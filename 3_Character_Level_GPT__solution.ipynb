{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Building a GPT from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a character-level **GPT (Generative Pretrained Transformer)** model from scratch. **Transformer** is the name of the underlying neural net architecture that was introduced in the 2017 groundbreaking paper \"Attention is All You Need\" (Link at the bottom).\n",
    "The model will be trained on different texts, for example Shakespeare, Goethe's \"Faust\", the \"Lord of the Rings\" or books from Jane Austen, and will be able to generate new text based on the text from the book.\n",
    "\n",
    "\n",
    "**NOTE:** You may answer in English or German.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-Loading-the-Data)\n",
    "\n",
    "[2. Tokenization](#2.-Tokenization)\n",
    "\n",
    "[3. Making Training Mini-Batches](#3.-Making-Training-Mini-Batches)\n",
    "\n",
    "[4. Defining the Network with PyTorch](#4.-Defining-the-Network-with-PyTorch)\n",
    "\n",
    "[5. Training](#5.-Training)\n",
    "\n",
    "[6. The Mathematical Trick in Self-Attention](#6.-The-Mathematical-Trick-in-Self-Attention)\n",
    "\n",
    "[7. Self-Attention](#7.-Self-Attention)\n",
    "\n",
    "[8. Full GPT Implementation](#9.-Full-GPT-Implementation)\n",
    "\n",
    "[9. Outlook and Next Steps](#9.-Outlook-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20d5d0e3c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# select the right file and read it in to inspect it\n",
    "# with open('text_input/faust.txt', 'r', encoding='utf-8') as f:\n",
    "with open('text_input/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('text_input/austen.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('text_input/LOTR.txt', 'r') as f:\n",
    "# with open('text_input/LOTR_TVscript.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1a) Find out the length of the dataset and print the first 1000 characters! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1b) Store all unique characters that occur in this text in `chars` and print them. Store the number of unique characters in `vocab_size` and print the result. **(3 points)**\n",
    "\n",
    "**Hint:** First make a set of all characters to remove duplicates, then make a list out of them to get a unique ordering, and finally sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size= 65\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size=', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to **tokenize** the input. This means, we convert the raw text string to some sequence of integers according to some **vocabulary** of possible elements. A **token** can be a character like here, or a piece of a word like in ChatGPT. For a character-level language model, we just translate each character to an integer (**encoding**) and vice-versa (**decoding**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2a) Test the code above by encoding some sentence of your choice and decoding it again. Print the encoded and decoded result. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokenization is a trade-off between vocabulary size and sequence length: Large vocabularies will lead to shorter encoding sequences and vice versa. For example, encoding each character results in a short vocabulary of 26 tokens for the standard alphabet plus some more for special characters, but each word consists of longer encodings. On the other hand, encoding on word level means each word is encoded as a single token, but the vocabulary will be much larger (up to a whole dictionary of hundreds of thousands of words for one language). In practice, for example in ChatGPT, **sub word encodings** are used, which means not encoding entire words, but also not encoding individual characters. Instead, some intermediate format is used, for example the word 'undefined' could be encoded as three tokens: 'un', 'define', 'd'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2b) Encode the entire text dataset and store it into a `torch.tensor` with `dtype=torch.long`. This will be our input data for the model, and we name it `data`. \n",
    "Print the shape and dtype of `data` and the first 1000 entries of `data` to see how the text above has been transformed. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# let's now encode the entire text dataset and store it into a torch.tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Making Training Mini-Batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) Split the data into 90% training and 10% validation data and store the result in `train_data` and `val_data`, respectively. We keep the validation data to detect overfitting: We don't want just a perfect memorization of this exact input text, we want a neural network that creates new text in a similar style. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only feed in chunks of data of size 8 here: feeding in all text at once is computationally too expensive. This is called the **block size** or **context length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # +1 because the target is the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `train_data` chunk of 9 characters, 8 training examples are hidden. Let's spell it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # this will be the input\n",
    "y = train_data[1:block_size+1] # this will be the target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides efficiency, a second reason to feed in chunks of size `block_size` is to make the Transformer be used to seeing contexts of different lengths, from only 1 token all the way up to `block_size` and every length in between. That is going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as one character of context and the Transformer knows how to predict the next character. Then it can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than block size inputs when it's predicting the next character.\n",
    "\n",
    "Besides the **time dimension** that we have just looked at, there is also the **batch dimension**: We feed in batches of multiple chunks of text that are all stacked up in a single tensor. This is simply done for efficiency, because the GPUs can process these batches in parallel.\n",
    "\n",
    "Now let's create random **batches** of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 (=batch_size) random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack 4 chunks (4x8 tensor) as rows in a minibatch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the same but one ahead (shifted 1 position to the right)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Get a batch of training data and store the inputs and targets in `xb` and `yb`, respectively. Print the results and their shapes. **(2 points)** \n",
    "\n",
    "**HINT:** Apply the `get_batch()` function above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) How many individual training examples for the transformer does this batch contain? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ANSWER:** This batch contains 4*8 examples that are completely independent for the transformer. Let's print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24] the target is: 43\n",
      "when input is [24, 43] the target is: 58\n",
      "when input is [24, 43, 58] the target is: 5\n",
      "when input is [24, 43, 58, 5] the target is: 57\n",
      "when input is [24, 43, 58, 5, 57] the target is: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "when input is [44] the target is: 53\n",
      "when input is [44, 53] the target is: 56\n",
      "when input is [44, 53, 56] the target is: 1\n",
      "when input is [44, 53, 56, 1] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58] the target is: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "when input is [52] the target is: 58\n",
      "when input is [52, 58] the target is: 1\n",
      "when input is [52, 58, 1] the target is: 58\n",
      "when input is [52, 58, 1, 58] the target is: 46\n",
      "when input is [52, 58, 1, 58, 46] the target is: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "when input is [25] the target is: 17\n",
      "when input is [25, 17] the target is: 27\n",
      "when input is [25, 17, 27] the target is: 10\n",
      "when input is [25, 17, 27, 10] the target is: 0\n",
      "when input is [25, 17, 27, 10, 0] the target is: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3d) Why do the targets look like this, where does the structure come from? What do we input to the transformer? **(2 points)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The targets `yb` are simply `xb` shifted by one character, because we predict the next character. The input to the transformer is `xb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Defining the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple bigram language model to start with, i.e., the model predicts the next character simply on the last character. This bigram model should look familiar from our first notebook! Only now, we implement a bigram model class inheriting from `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "loss= tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated text: \n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # e.g. if the input is token 5, the output should be the logits for all tokens at position 6 \n",
    "        # = the 5th row of the embedding table (see makemore video on bigram language model)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # pluck out the embeddings for the tokens in the input (=the row of the embedding table corresponding to its index) and interpret them as logits=scores\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch size=4, time=8, channels=vocab_size because we are predicting the probability of each token (vocab_size C) at each time step (block_size T) in each batch (batch_size B)\n",
    "\n",
    "        # if we have targets, compute the CE loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # need to reshape for CE-loss in PyTorch \n",
    "            # (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # same shape as logits\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions (ignore the loss because we don't have targets)\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step = prediction for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C) instead of (B, T, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we sample one token at a time for each batch\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print('loss=', loss) \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start with a single token = 0 (idx = current context)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "# generate operates on batch level -> index into the 0th row = single batch dimension that exists -> one-dimensional array of all the indices (time steps)\n",
    "# afterwards convert to simple python list from tensor for decode function\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Go through the class definition above and explain what each function does! (1-2 sentences per function) **(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The `__init__()` function creates an embedding table out of the inputs. The `forward()` function calculates a forward pass through the network: It simply plucks out the row of the embedding matrix corresponding to the input character and interprets it as logits (weighted sum). Then it calculates a cross entropy loss using the labels. The `generate()` function applies softmax to the logits to get class probabilities, then samples the next character from a multinomial distribution based on these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How do you interpret the generated text? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** We see that the output is still nonsense because the model is untrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) What loss do you expect for this model? Can you compare the actual loss with your expectation? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The loss should be `-log(1/vocab_size)`, e.g. in the Shakespeare dataset, `-log(1/64) = 4.16`. The actual loss is a bit higher with a value of 5.04. This is due to a bit of randomness, not all tokens are equally likely at initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until now, the text history is not used, it is a simple bigram model (only the last character is used to predict the next one). Still, we feed in the whole sequence `xb`, `yb` up to `block_size` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5a) Create a PyTorch Adam optimizer with a learning rate of `1e-3`, pass it the model parameters for optimization (`model.parameters()`) and store it in `optimizer`. Check the documentation if needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=4.658271312713623\n",
      "step=100, loss=4.5111260414123535\n",
      "step=200, loss=4.465275764465332\n",
      "step=300, loss=4.2778754234313965\n",
      "step=400, loss=4.135361671447754\n",
      "step=500, loss=4.1009368896484375\n",
      "step=600, loss=4.087919235229492\n",
      "step=700, loss=3.9148900508880615\n",
      "step=800, loss=3.8526744842529297\n",
      "step=900, loss=3.7584140300750732\n",
      "step=1000, loss=3.7861011028289795\n",
      "step=1100, loss=3.563880205154419\n",
      "step=1200, loss=3.5855166912078857\n",
      "step=1300, loss=3.4968693256378174\n",
      "step=1400, loss=3.436549186706543\n",
      "step=1500, loss=3.426408290863037\n",
      "step=1600, loss=3.3652169704437256\n",
      "step=1700, loss=3.327918767929077\n",
      "step=1800, loss=3.194262742996216\n",
      "step=1900, loss=3.2044880390167236\n",
      "step=2000, loss=3.1342856884002686\n",
      "step=2100, loss=3.034728527069092\n",
      "step=2200, loss=2.9980807304382324\n",
      "step=2300, loss=3.070113182067871\n",
      "step=2400, loss=2.962170362472534\n",
      "step=2500, loss=3.0024642944335938\n",
      "step=2600, loss=2.826655626296997\n",
      "step=2700, loss=2.960874319076538\n",
      "step=2800, loss=2.9202170372009277\n",
      "step=2900, loss=2.8009681701660156\n",
      "step=3000, loss=2.658357620239258\n",
      "step=3100, loss=2.7676682472229004\n",
      "step=3200, loss=2.7083563804626465\n",
      "step=3300, loss=2.6876325607299805\n",
      "step=3400, loss=2.6991372108459473\n",
      "step=3500, loss=2.757277250289917\n",
      "step=3600, loss=2.6523561477661133\n",
      "step=3700, loss=2.6728813648223877\n",
      "step=3800, loss=2.5633881092071533\n",
      "step=3900, loss=2.6789321899414062\n",
      "step=4000, loss=2.6050984859466553\n",
      "step=4100, loss=2.6432387828826904\n",
      "step=4200, loss=2.5973658561706543\n",
      "step=4300, loss=2.590667963027954\n",
      "step=4400, loss=2.5424563884735107\n",
      "step=4500, loss=2.5252456665039062\n",
      "step=4600, loss=2.5899834632873535\n",
      "step=4700, loss=2.5105459690093994\n",
      "step=4800, loss=2.549785614013672\n",
      "step=4900, loss=2.5883522033691406\n",
      "step=5000, loss=2.5792124271392822\n",
      "step=5100, loss=2.5681862831115723\n",
      "step=5200, loss=2.5436134338378906\n",
      "step=5300, loss=2.4788663387298584\n",
      "step=5400, loss=2.5035758018493652\n",
      "step=5500, loss=2.5601792335510254\n",
      "step=5600, loss=2.5641133785247803\n",
      "step=5700, loss=2.6006503105163574\n",
      "step=5800, loss=2.3605799674987793\n",
      "step=5900, loss=2.476731061935425\n",
      "step=6000, loss=2.5388400554656982\n",
      "step=6100, loss=2.492650270462036\n",
      "step=6200, loss=2.4032280445098877\n",
      "step=6300, loss=2.4595885276794434\n",
      "step=6400, loss=2.5007128715515137\n",
      "step=6500, loss=2.434014320373535\n",
      "step=6600, loss=2.5419490337371826\n",
      "step=6700, loss=2.509185552597046\n",
      "step=6800, loss=2.59481143951416\n",
      "step=6900, loss=2.540569543838501\n",
      "step=7000, loss=2.433148145675659\n",
      "step=7100, loss=2.4574191570281982\n",
      "step=7200, loss=2.532711982727051\n",
      "step=7300, loss=2.425440788269043\n",
      "step=7400, loss=2.4213614463806152\n",
      "step=7500, loss=2.3525350093841553\n",
      "step=7600, loss=2.436553716659546\n",
      "step=7700, loss=2.4609334468841553\n",
      "step=7800, loss=2.4691004753112793\n",
      "step=7900, loss=2.478074073791504\n",
      "step=8000, loss=2.4081239700317383\n",
      "step=8100, loss=2.515564203262329\n",
      "step=8200, loss=2.398622751235962\n",
      "step=8300, loss=2.5117411613464355\n",
      "step=8400, loss=2.4670135974884033\n",
      "step=8500, loss=2.429988384246826\n",
      "step=8600, loss=2.4367458820343018\n",
      "step=8700, loss=2.4522688388824463\n",
      "step=8800, loss=2.649188995361328\n",
      "step=8900, loss=2.3619937896728516\n",
      "step=9000, loss=2.496692657470703\n",
      "step=9100, loss=2.4796195030212402\n",
      "step=9200, loss=2.498892307281494\n",
      "step=9300, loss=2.442564010620117\n",
      "step=9400, loss=2.461780309677124\n",
      "step=9500, loss=2.3577675819396973\n",
      "step=9600, loss=2.537917137145996\n",
      "step=9700, loss=2.550713539123535\n",
      "step=9800, loss=2.4117798805236816\n",
      "step=9900, loss=2.4293642044067383\n",
      "2.5589075088500977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase batch size for better results\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # logits are not needed here\n",
    "    optimizer.zero_grad(set_to_none=True) # reset the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    # print the loss every 100 steps\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step={steps}, loss={loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate new text based on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "Ong h hasbe pave pirance\n",
      "RDe hicomyonthar's\n",
      "PES:\n",
      "AKEd ith henourzincenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KINld pe wither vouprroutherccnohathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so itJas\n",
      "Waketancotha:\n",
      "h hay.JUCLUKn prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZEESTEORDY:\n",
      "h l.\n",
      "KEONGBUCHandspo be y,-JZNEEYowddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghienHen yof GLANCHI me. strsithisgothers jveere!-e!\n",
      "QUCotouciullle's fld\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \")\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5b) How do you interpret the result? What could be a reason that the output is still suboptimal? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: The result looks a little bit better, but still suboptimal because we only used the last character for predicting the next, not the entire sequence. Next, all tokens should talk to each other for making better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized code so far (with some additions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "step 0: train loss 4.6715, val loss 4.6799\n",
      "step 300: train loss 2.8273, val loss 2.8328\n",
      "step 600: train loss 2.5454, val loss 2.5769\n",
      "step 900: train loss 2.5039, val loss 2.5136\n",
      "step 1200: train loss 2.4738, val loss 2.5101\n",
      "step 1500: train loss 2.4762, val loss 2.4967\n",
      "step 1800: train loss 2.4747, val loss 2.4883\n",
      "step 2100: train loss 2.4683, val loss 2.4861\n",
      "step 2400: train loss 2.4651, val loss 2.4890\n",
      "step 2700: train loss 2.4531, val loss 2.4977\n",
      "\n",
      "\n",
      "\n",
      "CExthy brid owindakis by ble\n",
      "\n",
      "Hisen bobe t e.\n",
      "S:\n",
      "O:\n",
      "ISA:\n",
      "\n",
      "\n",
      "LUCous:\n",
      "Wanthar usqur, vet?\n",
      "F dXENDoate awice my.\n",
      "\n",
      "HNEdarom oroup\n",
      "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heridrovets, and Win nghirileranousel lind te l.\n",
      "MAshe ce hiry:\n",
      "Supr aisspllw y.\n",
      "Hentous noroopetelaves\n",
      "MP:\n",
      "\n",
      "Pl, d mothakleo Windo whthCoribyo the m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar igist m:\n",
      "\n",
      "Thin inleronth,\n",
      "Mad re?\n",
      "\n",
      "WISo myr f-bube!\n",
      "KENob&isar adsal thes ghesthidin cour ay aney Iry ts I fr t ce.\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # new: check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # new: we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # new: average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # new: switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # new: switch back to train mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "---\n",
    "## 6. The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now derive a more complex model that can look at all tokens at once to predict the next one, not just the last token. \n",
    "To use all previous tokens, the simplest idea is to use an average of all previous tokens. \n",
    "For example, the 5th token uses the **channels** (=feature maps, embeddings) of the 1st, 2nd, 3rd, 4th, and 5th token. \n",
    "The average of these is the **feature vector** for the 5th token and summarizes the context / history.\n",
    "Note that we have lost a lot of information, e.g. the order of the tokens, but it's a starting point. Consider the following toy example with batch size 4 , 8 tokens, 2 channels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # batch, time, channels. Goal: 8 tokens should talk to each other, but only from previous tokens, not from future tokens\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in each batch in the example vector `x`, we calculate the mean of the tokens that came before it in the time dimension (including itself). \n",
    "The result should be a tensor of shape (B,T,C) where the t-th row of the b-th batch contains the mean of all tokens in this batch that came before this token in the time dimension.\n",
    "We print the original tensor `x` and the resulting tensor `xbow` containing the mean values and make sure the mean values are correct. Here `bow` stands for **bag of words**, which means that each entry is an average of several words (each of the 8 tokens is considered a 'word' here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words = simple average of all previous tokens\n",
    "for b in range(B): # iterate over batch dimension\n",
    "    for t in range(T): # iterate over time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) # all previous tokens for this batch and time (slice)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # mean over time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5208, -0.4545],\n",
       "        [ 0.5280, -0.5164],\n",
       "        [ 0.6794,  0.2099],\n",
       "        [ 0.3341, -1.1823],\n",
       "        [-1.3250, -0.5044],\n",
       "        [-0.8557,  0.0739],\n",
       "        [-0.6773,  0.9515],\n",
       "        [ 1.3192, -0.4420]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5208, -0.4545],\n",
       "        [ 1.0244, -0.4854],\n",
       "        [ 0.9094, -0.2537],\n",
       "        [ 0.7656, -0.4858],\n",
       "        [ 0.3475, -0.4895],\n",
       "        [ 0.1469, -0.3956],\n",
       "        [ 0.0292, -0.2032],\n",
       "        [ 0.1905, -0.2330]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # vertical average of all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using several nested loops like above, we use a trick with matrix multiplication that is mathematically equivalent but more efficient. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3) \n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, c contains the sum of the column entries of b. Because we only want the \"history\", not the \"future\" tokens to influence the result, we use a lower triangular matrix `a` instead, this is called **masking**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows, \n",
    "# third row is sum of all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to normalize for averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows + normalized, \n",
    "# third row is sum of all rows + normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6a) Now let's go back to our example above and apply the same trick. \n",
    "Define a lower triangular matrix called `wei` (previously `a`) that is normalized to sum to 1 along the rows. Matrix multiply `wei` with `x` to get a new matrix `xbow2`.\n",
    "Make sure that `xbow2` has the same shape as `x` and that it contains the correct values. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same result? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T)) # wei = weights (=a in the toy example)\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "# batched matrix multiplication (we apply matrix multiplication to each batch element separately)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) # c = a @ b in the toy example\n",
    "print('same result?', torch.allclose(xbow, xbow2)) # check if the two methods give the same result\n",
    "\n",
    "wei # average (sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5208, -0.4545],\n",
       "         [ 1.0244, -0.4854],\n",
       "         [ 0.9094, -0.2537],\n",
       "         [ 0.7656, -0.4858],\n",
       "         [ 0.3475, -0.4895],\n",
       "         [ 0.1469, -0.3956],\n",
       "         [ 0.0292, -0.2032],\n",
       "         [ 0.1905, -0.2330]]),\n",
       " tensor([[ 1.5208, -0.4545],\n",
       "         [ 1.0244, -0.4854],\n",
       "         [ 0.9094, -0.2537],\n",
       "         [ 0.7656, -0.4858],\n",
       "         [ 0.3475, -0.4895],\n",
       "         [ 0.1469, -0.3956],\n",
       "         [ 0.0292, -0.2032],\n",
       "         [ 0.1905, -0.2330]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xbow[0], xbow2[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6b) Now we use yet another mathematically equivalent way to compute the bag of words representation using **Softmax** function (this will be needed later for weighted sum instead of average of previous tokens).\n",
    "We start off with a lower triangular matrix where the lower triangle and diagonal is filled with 0, the upper with `-inf`. \n",
    "After applying the softmax function, the result will be again the `wei` matrix from before. Implement this in the following cell, calculate again the matrix multiplication of the new `wei` and `x` and check the result! **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix\n",
    "print(tril)\n",
    "\n",
    "wei = torch.zeros((T,T)) # preview: affinities are going to be dependent on context instead of zeroes\n",
    "# -> weighted aggregation in self-attention (tokens will find each other more or less interesting)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # make all future tokens -inf\n",
    "print(wei)\n",
    "\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize by row sum\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "print(wei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5208, -0.4545],\n",
       "         [ 1.0244, -0.4854],\n",
       "         [ 0.9094, -0.2537],\n",
       "         [ 0.7656, -0.4858],\n",
       "         [ 0.3475, -0.4895],\n",
       "         [ 0.1469, -0.3956],\n",
       "         [ 0.0292, -0.2032],\n",
       "         [ 0.1905, -0.2330]]),\n",
       " tensor([[ 1.5208, -0.4545],\n",
       "         [ 1.0244, -0.4854],\n",
       "         [ 0.9094, -0.2537],\n",
       "         [ 0.7656, -0.4858],\n",
       "         [ 0.3475, -0.4895],\n",
       "         [ 0.1469, -0.3956],\n",
       "         [ 0.0292, -0.2032],\n",
       "         [ 0.1905, -0.2330]]),\n",
       " tensor([[ 1.5208, -0.4545],\n",
       "         [ 1.0244, -0.4854],\n",
       "         [ 0.9094, -0.2537],\n",
       "         [ 0.7656, -0.4858],\n",
       "         [ 0.3475, -0.4895],\n",
       "         [ 0.1469, -0.3956],\n",
       "         [ 0.0292, -0.2032],\n",
       "         [ 0.1905, -0.2330]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Self-Attention\n",
    "\n",
    "Finally we get to the most important mechanism: **Self-Attention**! This will lead to a weighted average of the tokens (some tokens are more important than others to understand the text) instead of simply using the mean. And here is the idea: Every single token will emit two vectors: A **query** (\"What am I looking for?\") and a **key** (\"What do I contain?\"). The query then dot-products with all the keys to determine the similarity = affinity (stored in `wei`). Instead of the raw input `x`, which is private, a **value** is used (\"What will I communicate?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (increase channels for more interesting results)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # forward pass of x through the key layer\n",
    "q = query(x) # (B, T, 16) # forward pass of x through the query layer\n",
    "# so far, each token has a key and a query vector, no communication yet\n",
    "wei =  q @ k.transpose(-2, -1) # transpose last 2 dimensions (batch remains unchanged): (B, T, 16) @ (B, 16, T) ---> (B, T, T): for each batch, each token talks to all other tokens, so we get an affinity matrix of size (T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) # old version -> change to data dependent weights\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # comment to see intermediate results before normalization\n",
    "\n",
    "v = value(x) # we use the aggregated value instead of the raw x \n",
    "# x is private information to this token, v is the public information for communication\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) Print `wei` and compare it to the previous values. What is the most important change and why is this important here? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** Now `wei` is not constant anymore, but data dependent (every batch will have different weights because of different tokens)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the final entry 0.2391 is the weight for the 8th token. The 8th token emits a query ( for example \"I am a vowel at position 8, I am looking for consonants at positions up to 4\"). All tokens then emit keys, and maybe a consonant at position 4 will emit a key with high number in this channel, meaning \"I am a consonant at position 4\". The 8th token will therefore have a high weight for the 4th token (0.2297), resulting in a high affinity (dot product) - the 4th and 8th token \"have found each other\". Through the softmax function, a lot of information from the 4th token will be passed to the 8th token (meaning the 8th token will learn a lot from the 4th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "### Some Notes on Attention\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. Here we have `block_size = 8` nodes, where the first node is only pointed to by itself, the second by the first and itself, and so on. Attention can be applied to any directed graph, not only language modeling.\n",
    "- Each example across batch dimension is processed completely independently, the examples never \"talk\" to each other across different batches. The batched matrix multiplication above means applying matrix multiplication in parallel in each batch separately. For example here, you can think of 4 different graphs in parallel with 8 noded each, where the 8 nodes only communicate among each other, even though we process 32 nodes at once.\n",
    "- \"Scaled\" attention also divides `wei` by `1/sqrt(head_size)`, in the original paper:\n",
    "\\begin{equation*}\n",
    "   Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Without the normalization, using Gaussian input (zero mean and variance 1), the weights will be in the order of `head_size`. Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size) # k initialized from standard normal distribution (zero mean, unit variance)\n",
    "q = torch.randn(B,T,head_size) # q initialized from standard normal distribution (zero mean, unit variance)\n",
    "wei_unnormalized = q @ k.transpose(-2, -1) # will have variance of head_size roughly\n",
    "wei_normalized = q @ k.transpose(-2, -1)* head_size**-0.5 # normalize by sqrt of head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # variance of k: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() # variance of q: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4690)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(wei_unnormalized.var()) # variance of the dot product: roughly head_size=16\n",
    "print(wei_normalized.var()) # variance of the dot product: roughly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7b) Find out why this is important: Apply softmax to a tensor with entries around 0, then to another tensor with more extreme values. What happens? Write in the answer cell why we want to avoid this. **(2 points)**\n",
    "\n",
    "**HINT:** `torch.softmax()` expects an input specifying along which dimension to calculate the normalization (=which dimension should sum to 1), so you can pass `dim=-1` as second input for a 1D tensor. (See https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) # original example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2932e-04, 8.1630e-07, 1.7980e-02, 8.1630e-07, 9.8169e-01])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*20, dim=-1) # scaled by 20 -> output is more extreme, more confident, converging to one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The weights will feed into softmax. If we have rather big or small weights, the softmax will converge towards one-hot vectors, which means that we only use one input node and discard all others. This is not what we want, especially in the beginning when the network is untrained. So the variance should remain around 1 especially at initialization, meaning the weights should be fairly diffuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Encoding and Positional Encoding\n",
    "\n",
    "We will make one change on the token encoding: Previously, the `token_embedding_table` was of size `(vocab_size, vocab_size)`, which means we directly plucked out the logits from the embedding table. Now we want to introduce an intermediate layer (make the net bigger). Therefore, we introduce a new parameter `n_embd` for the number of embedding dimensions, for example we can choose 32 or 64 for this intermediate representation. So instead of logits, the `token_embedding_table` will give us **token embeddings**. These will be fed to a linear layer afterwards to get the logits:\n",
    "```\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary   \n",
    "```\n",
    "\n",
    "In the attention mechanism derived so far, there is no notion of space. Attention simply acts over a set of vectors. Remember that we can think of attention as a directed graph, where the nodes have no idea where they are positioned in a space. But space matters in text: For example, \"people love animals\" has a significantly different meaning than \"animals love people\", so the ordering of the words is very important. This is why we need to **positionally encode** tokens:\n",
    "So far, we have only encoded each token according to its identity `idx`. But we now also encode its position in a second embedding table: Each position from `0` to `block_size-1` will get its own embedding vector. This is the code snippet from the init function that we will implement below: \n",
    "```\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "```\n",
    "And here is a code snippet from the forward function, showing how integers from 0 to `block_size` are positionally encoded: \n",
    "```\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "```\n",
    "Right now, this is not useful yet, because we only use the last token in the Bigram model, so the position does not matter. But using attention, it will matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Single Self-Attention Head\n",
    "\n",
    "Now let's summarize the code so far and add a single self-attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "step 0: train loss 4.2272, val loss 4.2237\n",
      "step 500: train loss 2.6882, val loss 2.7151\n",
      "step 1000: train loss 2.5293, val loss 2.5306\n",
      "step 1500: train loss 2.4893, val loss 2.4842\n",
      "step 2000: train loss 2.4425, val loss 2.4499\n",
      "step 2500: train loss 2.4230, val loss 2.4356\n",
      "step 3000: train loss 2.4128, val loss 2.4248\n",
      "step 3500: train loss 2.3936, val loss 2.4271\n",
      "step 4000: train loss 2.3935, val loss 2.4096\n",
      "step 4500: train loss 2.3909, val loss 2.4098\n",
      "\n",
      "And thef bridcowr,\n",
      "This a, ber\n",
      "\n",
      "Hiset bobe ale.\n",
      "Sthr-ans mealilanss:\n",
      "Want he us hathe.\n",
      "War dtlas aten wice my.\n",
      "\n",
      "Hand pom oroupe owns\n",
      "Mtof isth bot mil ndill, aes iree sen cie lat Het drovets, and in poran iserans!\n",
      "el lind peal.\n",
      "--ser onchiry:\n",
      "Aupriness hew ye n'sto's norfopeeelaves\n",
      "Mom\n",
      "Il wod mothak\n",
      "Go Windo wher eiiby ow atit,\n",
      "CHive cen, ime st so mowr-xste\n",
      "\n",
      "Apk hanterthind son; igis! mef thin inle ont ffaf Pre?\n",
      "\n",
      "Whiom.\n",
      "\n",
      "HKINLIERLO,\n",
      "Sby ak\n",
      "Sadsad acihaghe my uin cour ayr tey Iry ts I fr af voun\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # new: increase number of iterations due to lower learning rate\n",
    "eval_interval = 500 \n",
    "learning_rate = 1e-3 # new: lower learning rate (self-attention is more complex)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out\n",
    "\n",
    "# new: single self-attention head\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out \n",
    "    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # new: dimensionality of embeddings changed to n_embd as intermediate layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # new: position embeddings\n",
    "        self.sa_head = Head(n_embd) # new: self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # new: linear layer for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to the last block_size tokens (because we now use position embeddings, which only contain the last block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreased a bit, but the result is still not great. We will introduce some more changes following the transformer paper for further improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "---\n",
    "## 8. Full GPT Implementation\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "First, we add **multi-head attention**, which is simply several attention heads running in parallel, then concatenating the result over the channel dimension. A **projection layer** combines the concatenated outputs from all heads into a single unified representation and projects back to the original pathway. Note that \"projection\" in the context of Transformer models refers to a linear transformation that can either maintain, reduce, or even increase the dimensionality of the data. \n",
    "\n",
    "Intuitive Explanation: It helps to have multiple communication channels because these tokens have a lot to talk about - they want to find the consonants, the vowels, the vowels just from certain positions etc. and so it helps to create multiple independent channels of communication to gather lots of different types of data and then decode the output.\n",
    "\n",
    "<img src=\"multi-head-attention.jpg\" width=\"200\">\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "So far, we directly calculated the logits after the attention block, but this was way too fast - intuitively \"the tokens looked at each other, but didn't really have time to think on what they found from the other tokens\". Therefore, we add a feedforward single layer followed by a ReLU nonlinearity. Both layers together are called the **Transformer Block**, where we combine **communication** (self-attention) with **computation** (feedforward layer). This is on a per token level: Each token independently looks at the other tokens, and once it has gathered all the data, it thinks on that data individually. We implement this in the `Block` class below. The transformer block gets repeated over and over again.\n",
    "\n",
    "<img src=\"transformer.jpg\" width=\"300\">\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Also note that the transformer architecture above contains **skip connections (residual connections)**: The network contains parallel paths (one with some computations, one with the identity as \"shortcut\") that are combined via additions. Additions are great for backpropagation because they distribute gradients equally to both branches, so there is a \"shortcut\" for the gradients to directly propagate from the output to the input of the network. This avoids the vanishing gradient problem especially in the beginning - the transformer blocks only get more influence over time.\n",
    "\n",
    "\n",
    "<img src=\"skip-connection.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "\n",
    "The transformer architecture uses **layer norm** (called \"Norm\" in the architecture image above), which is very similar to **batch norm**: Batch norm makes sure that across the batch dimension, any individual neuron has unit gaussian distribution (zero mean, unit standard deviation). In layer norm, we don't normalize the columns, but the rows, which normalizes over layers instead of over batches:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}\\cdot \\gamma + \\beta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copied from BatchNorm1d in makemore series)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # previous batch mean -> index changed from 0 to 1\n",
    "    xvar = x.var(1, keepdim=True) # previous batch variance -> index changed from 0 to 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # no running mean and variance buffers needed like in batch norm\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8a) Check if mean and standard deviation of rows and/or columns are normalized now! Write the result in the answer cell. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1469) tensor(0.8803)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# columns are not normalized anymore:\n",
    "print(x[:,0].mean(), x[:,0].std()) # mean,std of one feature across all batch inputs\n",
    "\n",
    "# rows are normalized instead:\n",
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** We can see that rows are normalized now instead of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that layer norm is usually applied before the self-attention and linear layer nowadays (unlike the original paper) - one of the very few changes of the transformer architecture during the last years, otherwise mostly the architecture remained unchanged. This is called the **pre-norm formulation**. So here is a code snippet used below showing the two layer norms we will implement, one before the self-attention and one before the linear layer:\n",
    "\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x)) # layer norm directly applied to x before self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # layer norm applied before linear layer\n",
    "```\n",
    "\n",
    "Finally, another layer norm is typically applied at the end of the Transformer and right before the final linear layer that decodes into vocabulary. \n",
    "\n",
    "The size of the layer norm is `n_embds=32` here, so this is a per token transformation, it just normalizes the features and makes them unit Gaussian at initialization. Because these layer norms contain gamma and beta as trainable parameters, the layer norm may eventually create outputs that are not unit Gaussian depending on the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model\n",
    "\n",
    "We now have all components together so that we can scale up the model and make it bigger. Therefore, we add a parameter `n_layer=4` to specify that we want 4 transformer blocks.\n",
    "\n",
    "We also add **dropout** to prevent overfitting: with 4 transformer blocks, the network is getting quite large now. Therefore, we randomly deactivate some connections to prevent them from becoming too dominant. Because the mask of what's being dropped out has changed every single forward backward pass, effectively we end up training an ensemble of sub-networks. At test time, everything is fully enabled and all of those sub-networks are merged into a single ensemble, making it more robust.\n",
    "\n",
    "\n",
    "<img src=\"dropout.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPT with Multi-Head Attention and Transformer Block\n",
    "\n",
    "We finally get to the full GPT code, adding all the components explained above!\n",
    "\n",
    "**TODO:** 8b) In the summarized code below, comment each line to make sure you have understood all GPT components! You may use support from ChatGPT or GitHub Copilot, but double-check the results and be able to explain it yourself. (Yes, this is tedious, but it will help you get an in-depth understanding of the full GPT architecture) **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "10.788929 M parameters\n",
      "\n",
      "==============\n",
      "step 0: train loss 4.3684, val loss 4.3590\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "SaVqulef&&;giKUCZo\n",
      "\n",
      "D?QZFFmlHHkPkBj!'VTSbmRDynJeCHXEU.n;O,h&MHKRP\n",
      "\n",
      "qkjne AURe!PV$!EAyC Dbu$IbDKIGEff.Ca-F$SrYHIarCbcV\n",
      "xwkOu'Ww.WvxFxwFfRdI.LIzfmSBaw?ia\n",
      "Ib,3O3lE;maEwTcGGqmFlxHmshIqmUrObTsrcXwFz?agwGpP\n",
      "\n",
      "==============\n",
      "step 500: train loss 1.9148, val loss 2.0104\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Sisnding surncr: amest to him the and somen\n",
      "The will leve of of tousark, and cable.\n",
      "KIF dewastays le is biose tat will nith fortughtit'd ans?\n",
      "Le:\n",
      "Fom my le, I shyince dor may magend\n",
      "Fromefike of therd\n",
      "\n",
      "==============\n",
      "step 1000: train loss 1.5542, val loss 1.7388\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "And VIVALINA:\n",
      "How now not bawight o'er wich make, away yetter,\n",
      "be deothernabe word make, our such rember.\n",
      "\n",
      "ServENERYn:\n",
      "I woman in the ind.\n",
      "Give me.\n",
      "No his, when dive or bat the baste? O muchus hart,\n",
      "A\n",
      "\n",
      "==============\n",
      "step 1500: train loss 1.4067, val loss 1.6229\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Maboure:\n",
      "Braces meet! childrew, as out me in he bad; I\n",
      "must not shall-mother; cride than or a diance, poil two heart\n",
      "Hither the own joy's did his thou sway: fairs kings\n",
      "Faming me, thou worth onewellow\n",
      "\n",
      "==============\n",
      "step 2000: train loss 1.3205, val loss 1.5518\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "You are askep'd men momb friet-remblusled; lack,\n",
      "And could be not.\n",
      "\n",
      "FirsY:\n",
      "More melantain of the wellike the old. Come, blood\n",
      "Hear master love hellowly you, and pomp.\n",
      "The world, well, I, for I would t\n",
      "\n",
      "==============\n",
      "step 2500: train loss 1.2583, val loss 1.5169\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "\n",
      "LADY NAREL:\n",
      "On Goes wed in my fault.\n",
      "\n",
      "QUEEN you mother, you to I have done:\n",
      "Why, best you so done whither? Go, ho! say, so I rap,\n",
      "May gore, now in King Aumerle?\n",
      "You are of York.\n",
      "Sound Margaret,--York\n",
      "\n",
      "==============\n",
      "step 3000: train loss 1.2090, val loss 1.5017\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "ISABELLA:\n",
      "'Twind some fears,\n",
      "As I st not have as Tnled to Rome,\n",
      "What have devour to heart the monst,\n",
      "To lost.\n",
      "\n",
      "CAMILLO:\n",
      "Go hour but, spir will dream stay.\n",
      "I dream not rad spoken, from, vile myself,\n",
      "Fo\n",
      "\n",
      "==============\n",
      "step 3500: train loss 1.1694, val loss 1.4886\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "By, I am.\n",
      "\n",
      "LARTIUS:\n",
      "I cry, and whether round it,\n",
      "And we are for you the gratest.\n",
      "\n",
      "Third Servingman:\n",
      "The act! O advice; thought the letter's laments\n",
      "Lived he is makes these punish in this cheek\n",
      "Of wher\n",
      "\n",
      "==============\n",
      "step 4000: train loss 1.1338, val loss 1.4866\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "Nay, after.\n",
      "\n",
      "Cloan:\n",
      "Well tell in the king of wide.\n",
      "\n",
      "ANGELO:\n",
      "For 'twere two?\n",
      "Messengther: what not so? 'Tis rescures:\n",
      "Their he is all things in the secure and be drown\n",
      "Are men half and upo\n",
      "\n",
      "==============\n",
      "step 4500: train loss 1.0984, val loss 1.4839\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "A Cpollows troks of the birth, the world which troop.\n",
      "See and George, yet welcome here, helcomes:\n",
      "The belly to die, if this.\n",
      "Let them to be received, they unroyalt beat\n",
      "Of justice makes hat, no lesset\n",
      "\n",
      "==============\n",
      "step 4999: train loss 1.0645, val loss 1.4783\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Before him cause to time\n",
      "What he teems of his image\n",
      "Of that worns that to this command it. The seconcition\n",
      "Privily to my root and for\n",
      "Withstal a cousin of steath, with smiles in thread it.\n",
      "What art se\n",
      "\n",
      "Final sample:\n",
      "\n",
      "BRUTUS:\n",
      "When be as bestless as holp,\n",
      "As 'twere there lives our cheers, as sees we'll ears.\n",
      "\n",
      "CLIFFORD:\n",
      "And in much that receives from my church?\n",
      "And bock my shame I have to make myself:\n",
      "Now the honours of this men of her is,\n",
      "And the numbers theat amize from my grieves.\n",
      "\n",
      "RICHMOND:\n",
      "No, forbear this admits the device earth.\n",
      "\n",
      "SLY:\n",
      "And I need lay to how my pace bulk offices?\n",
      "Well, sir, and to my sons that have succope\n",
      "As giving good Caius of the worn are,\n",
      "Till our fairs I'll burn skree in love.\n",
      "\n",
      "First Senator:\n",
      "Then gentleman gracious in!\n",
      "\n",
      "Second Senator:\n",
      "'Tis it a dragen?\n",
      "\n",
      "First Murderer:\n",
      "Mercures it you.\n",
      "\n",
      "Know or himselves.\n",
      "\n",
      "Senaper:\n",
      "You have been brief; they not them so.\n",
      "\n",
      "MENENIUS:\n",
      "He saws you his noble speech, so besad: he says\n",
      "'This first nature should hear youth, for you.'\n",
      "And meet my fightiers came mutinous to our piraps.\n",
      "Where, where they have all never lant.\n",
      "She stabs, hearset their gracess, temper,\n",
      "And on that helped Warwick, that were am Like all,\n",
      "And Revolt's dill the morest with and wear\n",
      "Than themselves, I begins him unto\n",
      "A molth braily and usurp'd. Thus stern here be true this blood!\n",
      "Thus Ravensman, from the dash of needs?\n",
      "Tybalt! Methink'st thou and my face for ashame,\n",
      "More had urtacle name men's complace with\n",
      "That vale your gainsays; with forely is wades here\n",
      "viltuous shoke and the shorest, deadly knife\n",
      "The first am one throngs, and seeming it, how strong's weeds,\n",
      "Will insulling, put as left out ours, they\n",
      "would wrong acquains that rung, my ancient\n",
      "Actizents, to appraise their foes.\n",
      "Go fetch, let me their offence; let us fury sit\n",
      "Of it book agains, Place of Essues\n",
      "Your tongue here, for much borson\n",
      "To you forsworn in attempt for course be her lib,\n",
      "Would you be good.\n",
      "\n",
      "First Senator:\n",
      "I here in my officers or Aummoniatence,\n",
      "And my looks to my strong-sovere's weep-shreen:\n",
      "Al't strike my body back and my neglect;\n",
      "Having for so let's cheek\n",
      "Of Citizens: my tradess straight in a shralling\n",
      "So is rottly honour away;\n",
      "In that make chase their own in bedeen sent,\n",
      "and Dem\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters version 1 - start here\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # how many training iterations?\n",
    "eval_interval = 500 # how often to evaluate the loss on train and val sets\n",
    "learning_rate = 1e-3 # learning rate\n",
    "eval_iters = 200 # how many samples to evaluate the loss on (result = mean over eval_iters)\n",
    "n_embd = 64 # embedding dimension\n",
    "n_head = 4 # number of heads in multi-head attention\n",
    "n_layer = 4 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate \n",
    "\n",
    "# hyperparameters version 2 - only uncomment when training on GPU\n",
    "#\"\"\"\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # how many training iterations?\n",
    "eval_interval = 500 # how often to evaluate the loss on train and val sets\n",
    "learning_rate = 3e-4 # learning rate\n",
    "eval_iters = 200 # how many samples to evaluate the loss on (result = mean over eval_iters)\n",
    "n_embd = 384 # embedding dimension\n",
    "n_head = 6 # number of heads in multi-head attention\n",
    "n_layer = 6 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate \n",
    "#\"\"\"\n",
    "\n",
    "# ------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
    "print('Running on device:',device) # print the device\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data # use the correct data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # sample random starting indices\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack them together\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # the targets are one token ahead\n",
    "    x, y = x.to(device), y.to(device) # move to GPU if available\n",
    "    return x, y # return the batch\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {} # store the results in this dictionary\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']: # evaluate both on train and val\n",
    "        losses = torch.zeros(eval_iters) # store the loss values\n",
    "        for k in range(eval_iters): # iterate over eval_iters\n",
    "            X, Y = get_batch(split) # get a batch\n",
    "            logits, loss = model(X, Y) # compute the loss \n",
    "            losses[k] = loss.item() # store the losses\n",
    "        out[split] = losses.mean() # store the mean loss in the dictionary\n",
    "    model.train() # switch back to train mode\n",
    "    return out # return the dictionary\n",
    "\n",
    "class Head(nn.Module): \n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size): # head_size = n_embd // n_head\n",
    "        super().__init__() # initialize the module\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularization\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        wei = self.dropout(wei) # apply dropout for regularization\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out # return the result\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size): \n",
    "        super().__init__() # initialize the module\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # create num_heads heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # project back to the original pathway\n",
    "        # Note: \"projection\" in the context of Transformer models refers to a linear transformation\n",
    "        # that can either maintain, reduce, or even increase the dimensionality of the data.\n",
    "        # The projection layer combines the concatenated outputs from all heads into a single unified representation.\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularization\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenate output of each head\n",
    "        out = self.dropout(self.proj(out)) # project back to the original pathway and apply dropout\n",
    "        return out # return the result\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd): # n_embd: embedding dimension (e.g., 64 or 384)\n",
    "        super().__init__() # initialize the module\n",
    "        self.net = nn.Sequential( # simple feed forward network\n",
    "            nn.Linear(n_embd, 4 * n_embd), # grow the hidden layer by a factor of 4 (see original paper)\n",
    "            nn.ReLU(), # ReLU activation function\n",
    "            nn.Linear(4 * n_embd, n_embd), # project back to the original pathway\n",
    "            nn.Dropout(dropout), # dropout layer for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        return self.net(x) # return the result\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head): # n_embd: embedding dimension, n_head: the number of heads we'd like \n",
    "        super().__init__() # initialize the module\n",
    "        head_size = n_embd // n_head # each head will have this size because we split the embedding into n_head parts\n",
    "        # (typical transformer structure: n_embd is divisible by n_head)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # self-attention layer\n",
    "        self.ffwd = FeedFoward(n_embd) # feed forward layer\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # layer norm for the self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd) # layer norm for the feed forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # typical transformer block: multi-head attention followed by feed forward including skip connections\n",
    "        # note that layer norm is applied before the self-attention nowadays (unlike the original paper)\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection + layer norm + self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection + layer norm + feed forward\n",
    "        return x\n",
    "\n",
    "# language model now using attention \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\" GPT language model with self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize the module\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # stack of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm (typically added after the transformer stack, right before the final linear layer that decodes into the vocabulary)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "        B, T = idx.shape # batch size and time\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) - get token embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1 for position embeddings\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        x = self.blocks(x) # (B,T,C) - transformer block\n",
    "        x = self.ln_f(x) # (B,T,C) - final layer norm\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size) - predict the logits\n",
    "\n",
    "        if targets is None: # during inference we only need the logits\n",
    "            loss = None # loss is therefore None\n",
    "        else:\n",
    "            B, T, C = logits.shape # store the shape of the logits\n",
    "            logits = logits.view(B*T, C) # reshape (B,T,C) -> (B*T,C) for cross entropy to work (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # reshape targets similarly\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy loss\n",
    "\n",
    "        return logits, loss # return the logits and loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # generate from the model\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # generate max_new_tokens\n",
    "            idx_cond = idx[:, -block_size:] # crop idx to the last block_size tokens (will run out of memory if bigger than block_size)\n",
    "            logits, loss = self(idx_cond)  # get the predictions\n",
    "            logits = logits[:, -1, :] # (B, C) - focus only on the last time step\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) - apply softmax to get probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) - sample from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) - append sampled index to the running sequence\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = GPTLanguageModel() # create the model\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # print the number of parameters in the model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # create a PyTorch optimizer\n",
    "\n",
    "for iter in range(max_iters): # iterate over the dataset\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1: #every once in a while evaluate the loss on train and val sets\n",
    "        losses = estimate_loss() # estimate the loss\n",
    "        print(\"\\n==============\")\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # print the results\n",
    "        print(\"==============\")\n",
    "        \n",
    "        # generate from the model\n",
    "        print(\"\\nSample:\")\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available (start with a single token = 0)\n",
    "        print(decode(model.generate(context, max_new_tokens=200)[0].tolist())) # generate a sample wit 200 tokens and decode it\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train') \n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # forward pass of the input through the model\n",
    "    optimizer.zero_grad(set_to_none=True) # set the gradients to zero\n",
    "    loss.backward() # backward pass to compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "# generate from the model\n",
    "print(\"\\nFinal sample:\") # generate a sample with 2000 tokens\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available (start with a single token = 0)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist())) # generate a sample with 2000 tokens and decode it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have trained a more powerful GPT model using self-attention. Let's generate a longer text and see how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lady:\n",
      "Who says are makes amen.\n",
      "\n",
      "GLOUCESTER:\n",
      "So brother's good wife?  not so far, now.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "No, neithers, lords.\n",
      "\n",
      "KING RICHARD III:\n",
      "Art tire; for there the devil at the city.\n",
      "\n",
      "GLOUCESTER:\n",
      "To patient fears them that thiness old of quine,\n",
      "And tell is it to be yield.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Not good coming my way.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Did up, trumpet! it shall be us too star'd.\n",
      "\n",
      "YORK:\n",
      "Then besides all waiting by the Tower,\n",
      "Or what will I show, ere device\n",
      "Still by darments, the most toill to your evil use\n",
      "He thither I did not weigh in sin:--\n",
      "I through it should leave,--my follow's in our neck,\n",
      "And my sons weddest in iman-gain,\n",
      "I shall excrow your queen on.\n",
      "\n",
      "LUCIO:\n",
      "O your vicely, if you have wear eye of little more\n",
      "Is hugging.\n",
      "\n",
      "LUCIO:\n",
      "Such hasopiniorer, talks smoke the pricks.\n",
      "I should besief sool, a next, you do enter form!\n",
      "\n",
      "BRUTUS:\n",
      "That  Cholers so here, with noble course.\n",
      "\n",
      "CORIOLANUS:\n",
      "Answear me, rail, I'll cannot low.\n",
      "I have, I saw the cound of my backness,\n",
      "And might for my gazes death: they fight--\n",
      "\n",
      "SICANINIUS:\n",
      "I am made post, if to be yourself?\n",
      "\n",
      "CORIOLANUS:\n",
      "She she was not book'd: so much in secrifice\n",
      "I, what's yourough! prither!--there's best you?\n",
      "\n",
      "First Murderer:\n",
      "Some shows it shall still.\n",
      "\n",
      "MERCUTIO:\n",
      "This is yourself it will enough.\n",
      "\n",
      "ABENVALIO:\n",
      "There's none in your bosom.\n",
      "'These satisface with you and unspoken them, ney: have yrk.\n",
      "\n",
      "on; say 'I cannot call, what my cousin, sir, I did to give\n",
      "my sir, honest for a ramen casenous thing\n",
      "is beg the houraged stuff; but there's nar these\n",
      "have you hatch'd: you this, but you'ld but no your mind.\n",
      "\n",
      "LUCIO:\n",
      "Thou'rt these wars wind with you.\n",
      "\n",
      "MARIANA:\n",
      "I would to be chunted to hop: he is bawn's matter, and\n",
      "say brings, and ho's--\n",
      "\n",
      "ESCALUS:\n",
      "But beside my word: here he has gone!\n",
      "Not the duke is my tongue proved.\n",
      "\n",
      "LUCIO:\n",
      "Having your remember, sir: I am whence, are gleriod\n",
      "To do at voices I have tickly he; and, that's\n",
      "not goes\n",
      "More can conquined to my crown,\n",
      "And my secure will stir my now.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Come, sir, were tongue flunt,\n",
      "Achieven in their svain chrush.\n",
      "\n",
      "Provost:\n",
      "Grace me, as do go; take them over taulterest send you.\n",
      "My sight is guard: poor-head! the oney,\n",
      "hope of Thus is coming.\n",
      "\n",
      "DUKE's Romans, it was scious pamil'd up: and knows our their sin!\n",
      "\n",
      "PUKINGHAM:\n",
      "Spleen, it in Aum! Some prickets yields.\n",
      "We must end by the dear o'er that give leave?\n",
      "Is it not a popular; fleshing his eyes daughters\n",
      "With 'MPRecurely sighs were an embrone,\n",
      "Sinceth to coward all men: persons day omine,\n",
      "The obsent to make by the causets and age me\n",
      "And in Polixenes: beseech you his\n",
      "At is this enemies to mean: we'll I\n",
      "Live the treme not imported jury moves.\n",
      "\n",
      "MISTRESS OVERDONE:\n",
      "Three your general conceivers are\n",
      "Must youfan virtuous here.\n",
      "\n",
      "POMPEY:\n",
      "So pass'd it her, Why, he may read look it.\n",
      "\n",
      "ESCALUS:\n",
      "Let us your wisk; but all by.\n",
      "\n",
      "MISTRESS OVERESS OVen, my poor Clarence! Aufiddius\n",
      "IC.\n",
      "\n",
      "MAMILLIUS:\n",
      "I trust;\n",
      "You shall, pass not here the\n",
      "Small spoke no harmour.\n",
      "\n",
      "MARIANA:\n",
      "My gentleman were the prive?\n",
      "\n",
      "EENCEOMINIUS:\n",
      "As my entremesness\n",
      "Yondeed son people.\n",
      "\n",
      "ANGELO:\n",
      "Opening hurt go: but another my\n",
      "graces, and for forth it off my son: through both,\n",
      "You'll mode, twice humount: my wigh and my ener.\n",
      "Go you have you now, nor ricle to sing hold\n",
      "Bucklack marsters.\n",
      "\n",
      "BASHOP Officer:\n",
      "Ah, my good lord,\n",
      "I'll not burning. In this eye,\n",
      "And I being at this.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "A vault shall these men,\n",
      "Upon it justices put on! 'His gone;\n",
      "Nor none but it with it.\n",
      "\n",
      "LUCENTIO:\n",
      "Nay, you will\n",
      "To call the more than to the peoples of fire.'\n",
      "Provost, Verona! Therefore, traitor!\n",
      "Thou canst it from pardonal thence were this:\n",
      "Art strew'st, one, thou answer to him!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Bid this is ear; and what, if we be much\n",
      "Upon the ghoss that from you?\n",
      "His nece;\n",
      "But shall I half my preserved courses;\n",
      "And where you here; this is yourselvel,\n",
      "Shall I be in this some, when he Mas man as have\n",
      "To bow--fold manight to't calls. Or haild.\n",
      "How comes them to me, considered\n",
      "sirest on on them! and cannot a most or souls\n",
      "Proclaim at home, gentleman one;\n",
      "For do there will go, supply they rumping state\n",
      "Their like brisings names of pirocles,\n",
      "Travils and eyes, but what these two larks\n",
      "Montal use dut the perseath ones forth their\n",
      "And with all promise.\n",
      "\n",
      "Messenger:\n",
      "To make your triments,\n",
      "As we should there shall.\n",
      "\n",
      "LEONTES:\n",
      "No more of time, from some fremitation;\n",
      "You'rt not--must I must be constrel:\n",
      "They swear it,--less ere I'll retire any malice.\n",
      "\n",
      "HASTINGS MOTSLY:\n",
      "Patre your state, we are sorrity\n",
      "Rumvent, too rouble the steel about away.\n",
      "\n",
      "CAMILLO:\n",
      "AL, at the same from Maspers, and overt your gourace.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I will try without bawd-gone. I perform'd\n",
      "Now both yourself: by my wife, that\n",
      "To get in all most worsely contrated by that Makes of\n",
      "Attender than you'reed: he could still only star\n",
      "A sweeter vast reigh them mortalshedier with them!\n",
      "Master, to much best thanks by-judgment on molths,\n",
      "That thou spy-trilles child more twent then\n",
      "Whose coals to thy chamber?\n",
      "If we but shall this mouth with me so brittle:\n",
      "But three glues therefore them than they in their last,\n",
      "One moneral of the ground towards Isawabrance\n",
      "To brind his life and bmourn in my embraves\n",
      "Agates: I'll forget my infury\n",
      "These implorent valourish'd Delphed: was depass'd it,\n",
      "Fall there too, by my repaired pray.\n",
      "Why edge thee,--My sons! Thy heaven shoonis!\n",
      "Vnature 'n your offence made it, Claudio!'\n",
      "Wrong bey! So anon. Some honours, we did all to follow\n",
      "To sit feel and Bonaglano,\n",
      "To be convile.\n",
      "Give me for a bird of gover.\n",
      "\n",
      "BRUTUS:\n",
      "Here were in your noting.\n",
      "\n",
      "SICINIUS:\n",
      "Peace! iO her honour.\n",
      "Cousin:\n",
      "Some, that sithes, you is behold's place. How you pupklicipably you.\n",
      "\n",
      "CORIOLANUS:\n",
      "Standably uf much.\n",
      "\n",
      "COMINIUS:\n",
      "Our MAMNIUS:\n",
      "Verity!\n",
      "\n",
      "KING RICHARD III:\n",
      "Nay, Clifford; but then he, my lord fortunes;\n",
      "Then be sound to such a welcome of laxs\n",
      "And after innation; you'll do: lit buyld me\n",
      "And shall I now, come with you.\n",
      "\n",
      "MENENIUS:\n",
      "Here, ANRE:\n",
      "Before I'll be gone to see.\n",
      "Bear itsembly!\n",
      "Good grow! get, yester fance thy body are again.\n",
      "Delics upon tready! keep you much and soldier:\n",
      "Your general ghost waulined\n",
      "Upon to the earth, you call, you'll hear some a joy\n",
      "Of twains i' the cold forbid!\n",
      "Here's no sleep, and you speak: you're not out a grave,\n",
      "Anough'd to beg toward to begin, above you,\n",
      "As dare it yet well excellend in the sky frown;\n",
      "The before of Camillo of Frence bore to make,\n",
      "In ready the heir traitor makes of your roar:\n",
      "So the robbe his feeling: then, for poor.\n",
      "\n",
      "SICINIUS:\n",
      "Let them, be your father's, and they\n",
      "wear best both ever'd to between; marry weeps\n",
      "On the court of the shift country.\n",
      "\n",
      "First Senator:\n",
      "O, Isabella,\n",
      "My fates so the door sovereign presume of no,\n",
      "The flesh and whereof she shall power manner in you.\n",
      "\n",
      "Shepherd:\n",
      "Verily, all post she will have.\n",
      "\n",
      "Gentleman:\n",
      "And I would I had cause us to heal.\n",
      "\n",
      "All:\n",
      "Be comfort.\n",
      "\n",
      "LARTIUS:\n",
      "He drivered to infamity,\n",
      "And to big heaven of them toward to fire\n",
      "As fine shall helmse still to speak as change\n",
      "To hear himselves, and in his wavers of lame\n",
      "As the wildes eyes for all. To see her colour!\n",
      "\n",
      "COMINIUS:\n",
      "He would acmons her hands of and ceased\n",
      "With a sestice more aim,--preventefended impress'd current\n",
      "Our bears, Henry tlaster. But in truth once!'\n",
      "Such a coronation of our worth tock.\n",
      "When the outsic of his wife, my poart,\n",
      "And with some wrath, o' the morpairous from bears,\n",
      "To make my counsel: 'tis not one that force\n",
      "For him embraced by smirth to informents, and thence:\n",
      "The beauty I have some I cannot seen\n",
      "To put your rags' secrate. Come me to talk.\n",
      "\n",
      "BRUTUS:\n",
      "Say, repetite:\n",
      "What apple claudito?isanc!\n",
      "\n",
      "BRUTUNCE:\n",
      "You banishment, cheaping shall not. Live you;\n",
      "And you have realment. Camillo,\n",
      "Vowere your gone, your minded consuls gentler,\n",
      "Your royal Var; I cannot say,\n",
      "Your honour to be a king;\n",
      "Or, my good mercy, then so: yet must I ne'er\n",
      "gave you, craveless. But\n",
      "Your passans:\n",
      "You'll forecent; you must no marcel\n",
      "Lest use your sues, dunn'st acquaint him. Come, go you\n",
      "Your easures togar.\n",
      "\n",
      "CAMILLO:\n",
      "Ssee me one, it fares out toills.\n",
      "\n",
      "MENENIUS:\n",
      "What servants hours\n",
      "Commanded estomation, he will conterquire you of mine:\n",
      "Not She learn that were purpose. You have nones\n",
      "Many done: I'll give alone\n",
      "I bear my sins; and you mule speak, ho's mean in.\n",
      "\n",
      "COMINIUS:\n",
      "Give him, we'll be my content to know.\n",
      "\n",
      "COMINIUS:\n",
      "I should, sir, thou soundly for evar:\n",
      "I will speak to die.\n",
      "\n",
      "MENENIUS:\n",
      "Of I think that, he'll send good us.\n",
      "\n",
      "MISTINA:\n",
      "Nay, in hear me hence, and sir, wherein thou look'd\n",
      "WiPpeace: with you, or fail, craves his penity;\n",
      "Then it, but only on't.\n",
      "\n",
      "BRUTUS:\n",
      "How goes shall and heavens down. His\n",
      "deserves, and he would science more\n",
      "As if his man's country with his feeking wises,\n",
      "A she bids determs make same of kings\n",
      "And with some paple, to wink coals,\n",
      "And keps the worshed will envious makes a him.\n",
      "I'll do this day to sheapher he be meal'd.\n",
      "The baiteful body's improadent, I must go blay;\n",
      "And yet her nature not, cherish; if no never\n",
      "To she was for the returness or chadition.\n",
      "I'll forgive my knee: when I forew abroad;\n",
      "So that we may ready on town\n",
      "To calumn again. Follow. Here's he it without his\n",
      "Arthly days husbaned: sat therefore come ever,\n",
      "Ere ever wom? Therefore. Guess glad here:\n",
      "How now, King Richard herest power to bid,\n",
      "Even his sights; therefore present Servant, forbids\n",
      "Doth with all my only!\n",
      "Or cheaps, that he from the souts,\n",
      "This dogs have borne fitter before thee.\n",
      "\n",
      "ANGELO:\n",
      "What from this iron to sickly should come,\n",
      "Is not to bear the embracement.\n",
      "Now what both he his.\n",
      "\n",
      "ANGELO:\n",
      "Monducing for honest of mine aris chance:\n",
      "Nine, say he would buy a lady's comments;\n",
      "To see how his place the hour and armains,\n",
      "And between not those sore was forsaked hath\n",
      "And his practisement claim. Therefore she\n",
      "The dower of the worse: but I will not fool-book,\n",
      "Your hands forsworn oath and his proves not brothers;\n",
      "Nor my chamberlains an of York deperately;\n",
      "And Nemies to her brelthen you shall move\n",
      "In me hath alll your leisured\n",
      "He corrections from your and peace of heaven.\n",
      "Like to Rommen turtier in my breath\n",
      "And to my chill should pract it.\n",
      "\n",
      "HORTENSIO:\n",
      "I am grace to find Petruch\n",
      "Herefore. Lis your grace in heavens;\n",
      "But to purz\n"
     ]
    }
   ],
   "source": [
    "# generate a longer sample\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a text file\n",
    "f =  open(\"text_output/GPT_generated_text.txt\",\"w\")\n",
    "f.write(new_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Apply the code to a different text of your choice! What loss do you achieve? What parameters did you change and why? How do you interpret the output compared to the Shakespeare output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Next Steps\n",
    "### Andrej's Suggested Further Experiments\n",
    "\n",
    "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder and Encoder\n",
    "\n",
    "Text generation as above only uses the **decoder** part of the transformer architecture. The **decoder attention block** implemented above has **triangular masking**, and is usually used in autoregressive settings, like language modeling. \n",
    "\n",
    "In other settings, we do want \"future\" tokens to influence the prediction, so we do not use triangular masking. For example, in sentiment analysis, we look at a whole sentence at once, then predict the sentiment \"happy\" or \"sad\" of the speaker. This can be realized using an **encoder** attention block. To implement an encoder attention block, we can simply delete the single line that does masking with `tril`, allowing all tokens to communicate. Attention does not care whether tokens from the future contribute or not, it supports arbitrary connectivity between nodes.\n",
    "\n",
    "### From Self-Attention to Cross-Attention\n",
    "\n",
    "**Self-attention** means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module). For example, when translating from French to English, we condition the decoding on the past decoding *and* the fully encoded french prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GPT to ChatGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a long way to go from our toy GPT example to ChatGPT. \n",
    "First of all, ChatGPT's **pre-training** was done on a large chunk of internet, resulting in a decoder-only transformer for text generation. \n",
    "So the pretraining is quite similar to our toy example training, except that we used roughly 10 million parameters and the largest transformer for ChatGPT uses 175 billion (!) parameters. Also it was trained on 300 billion tokens (our training set would be 300.000 tokens roughly when not using character-level tokens, but sub-word chunks). This is about a million fold increase in number of tokens - and today, even bigger datasets are used with trillions of tokens for training on thousands of GPUs!\n",
    "\n",
    "See the following table for the number of parameters, number of layers, n_embd, number of heads, head size, batch size and learning rate in **GPT-3**:\n",
    "\n",
    "\n",
    "<img src=\"GPT3_params_table.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-training, the model will be a document completer, it will not give answers but produce more questions or result in some undefined behavior. For becoming an assistant, further **fine-tuning** is needed using **Reinforcement Learning from Human Feedback (RLHF)**. Here is an overview of manual fine-tuning with human AI trainers (see the OpenAI ChatGPT blog for details, link below):\n",
    "\n",
    "<img src=\"chatgpt_diagram_light.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To sum it up, we trained a decoder only Transformer following the famous paper 'Attention is All You Need' from 2017, which is basically a GPT. We saw how using self-attention, we can calculate a weighted average of past tokens to predict the next token. We trained it on different texts (Shakespeare, Faust, Jane Austen etc.) and produced new texts in the same writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 \n",
    "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in 3_Character_Level_GPT__solution.ipynb above threshold seen and so a NEW version has been made: `TRUNCATED_3_Character_Level_GPT__solution.ipynb`.\n"
     ]
    }
   ],
   "source": [
    "# This cell truncates long output to a maximum length, then converts the notebook to a PDF for submission\n",
    "# NOTE: You may have to adapt the path or filename to match your local setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# truncate long cell output to avoid large pdf files\n",
    "from helpers.truncate_output import truncate_long_notebook_output\n",
    "truncated = truncate_long_notebook_output('3_Character_Level_GPT__solution.ipynb')\n",
    "\n",
    "# convert to pdf with nbconvert\n",
    "if truncated:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download TRUNCATED_3_Character_Level_GPT__solution.ipynb\n",
    "else:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download 3_Character_Level_GPT__solution.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
