{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Building a GPT from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a character-level **GPT (Generative Pretrained Transformer)** model from scratch. **Transformer** is the name of the underlying neural net architecture that was introduced in the 2017 groundbreaking paper \"Attention is All You Need\" (Link at the bottom).\n",
    "The model will be trained on different texts, for example Shakespeare, Goethe's \"Faust\", the \"Lord of the Rings\" or books from Jane Austen, and will be able to generate new text based on the text from the book.\n",
    "\n",
    "\n",
    "**NOTE:** You may answer in English or German.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-Loading-the-Data)\n",
    "\n",
    "[2. Tokenization](#2.-Tokenization)\n",
    "\n",
    "[3. Making Training Mini-Batches](#3.-Making-Training-Mini-Batches)\n",
    "\n",
    "[4. Defining the Network with PyTorch](#4.-Defining-the-Network-with-PyTorch)\n",
    "\n",
    "[5. Training](#5.-Training)\n",
    "\n",
    "[6. The Mathematical Trick in Self-Attention](#6.-The-Mathematical-Trick-in-Self-Attention)\n",
    "\n",
    "[7. Self-Attention](#7.-Self-Attention)\n",
    "\n",
    "[8. Full GPT Implementation](#9.-Full-GPT-Implementation)\n",
    "\n",
    "[9. Outlook and Next Steps](#9.-Outlook-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x150c2a27270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# select the right file and read it in to inspect it\n",
    "# with open('faust.txt', 'r', encoding='utf-8') as f:\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('austen.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('LOTR.txt', 'r') as f:\n",
    "# with open('LOTR_TVscript.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1a) Find out the length of the dataset and print the first 1000 characters! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1b) Store all unique characters that occur in this text in `chars` and print them. Store the number of unique characters in `vocab_size` and print the result. **(3 points)**\n",
    "\n",
    "**Hint:** First make a set of all characters to remove duplicates, then make a list out of them to get a unique ordering, and finally sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size= 65\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size=', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to **tokenize** the input. This means, we convert the raw text string to some sequence of integers according to some **vocabulary** of possible elements. A **token** can be a character like here, or a piece of a word like in ChatGPT. For a character-level language model, we just translate each character to an integer (**encoding**) and vice-versa (**decoding**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2a) Test the code above by encoding some sentence of your choice and decoding it again. Print the encoded and decoded result. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokenization is a trade-off between vocabulary size and sequence length: Large vocabularies will lead to shorter encoding sequences and vice versa. For example, encoding each character results in a short vocabulary of 26 tokens for the standard alphabet plus some more for special characters, but each word consists of longer encodings. On the other hand, encoding on word level means each word is encoded as a single token, but the vocabulary will be much larger (up to a whole dictionary of hundreds of thousands of words for one language). In practice, for example in ChatGPT, **sub word encodings** are used, which means not encoding entire words, but also not encoding individual characters. Instead, some intermediate format is used, for example the word 'undefined' could be encoded as three tokens: 'un', 'define', 'd'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2b) Encode the entire text dataset and store it into a `torch.tensor` with `dtype=torch.long`. This will be our input data for the model, and we name it `data`. \n",
    "Print the shape and dtype of `data` and the first 1000 entries of `data` to see how the text above has been transformed. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# let's now encode the entire text dataset and store it into a torch.tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Making Training Mini-Batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) Split the data into 90% training and 10% validation data and store the result in `train_data` and `val_data`, respectively. We keep the validation data to detect overfitting: We don't want just a perfect memorization of this exact input text, we want a neural network that creates new text in a similar style. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only feed in chunks of data of size 8 here: feeding in all text at once is computationally too expensive. This is called the **block size** or **context length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # +1 because the target is the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `train_data` chunk of 9 characters, 8 training examples are hidden. Let's spell it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # this will be the input\n",
    "y = train_data[1:block_size+1] # this will be the target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides efficiency, a second reason to feed in chunks of size `block_size` is to make the Transformer be used to seeing contexts of different lengths, from only 1 token all the way up to `block_size` and every length in between. That is going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as one character of context and the Transformer knows how to predict the next character. Then it can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than block size inputs when it's predicting the next character.\n",
    "\n",
    "Besides the **time dimension** that we have just looked at, there is also the **batch dimension**: We feed in batches of multiple chunks of text that are all stacked up in a single tensor. This is simply done for efficiency, because the GPUs can process these batches in parallel.\n",
    "\n",
    "Now let's create random **batches** of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 (=batch_size) random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack 4 chunks (4x8 tensor) as rows in a minibatch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the same but one ahead (shifted 1 position to the right)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Get a batch of training data and store the inputs and targets in `xb` and `yb`, respectively. Print the results and their shapes. **(2 points)** \n",
    "\n",
    "**HINT:** Apply the `get_batch()` function above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) How many individual training examples for the transformer does this batch contain? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ANSWER:** This batch contains 4*8 examples that are completely independent for the transformer. Let's print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24] the target is: 43\n",
      "when input is [24, 43] the target is: 58\n",
      "when input is [24, 43, 58] the target is: 5\n",
      "when input is [24, 43, 58, 5] the target is: 57\n",
      "when input is [24, 43, 58, 5, 57] the target is: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "when input is [44] the target is: 53\n",
      "when input is [44, 53] the target is: 56\n",
      "when input is [44, 53, 56] the target is: 1\n",
      "when input is [44, 53, 56, 1] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58] the target is: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "when input is [52] the target is: 58\n",
      "when input is [52, 58] the target is: 1\n",
      "when input is [52, 58, 1] the target is: 58\n",
      "when input is [52, 58, 1, 58] the target is: 46\n",
      "when input is [52, 58, 1, 58, 46] the target is: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "when input is [25] the target is: 17\n",
      "when input is [25, 17] the target is: 27\n",
      "when input is [25, 17, 27] the target is: 10\n",
      "when input is [25, 17, 27, 10] the target is: 0\n",
      "when input is [25, 17, 27, 10, 0] the target is: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3d) Why do the targets look like this, where does the structure come from? What do we input to the transformer? **(2 points)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The targets `yb` are simply `xb` shifted by one character, because we predict the next character. The input to the transformer is `xb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Defining the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple bigram language model to start with, i.e., the model predicts the next character simply on the last character. This bigram model should look familiar from our first notebook! Only now, we implement a bigram model class inheriting from `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "loss= tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated text: \n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # e.g. if the input is token 5, the output should be the logits for all tokens at position 6 \n",
    "        # = the 5th row of the embedding table (see makemore video on bigram language model)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # pluck out the embeddings for the tokens in the input (=the row of the embedding table corresponding to its index) and interpret them as logits=scores\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch size=4, time=8, channels=vocab_size because we are predicting the probability of each token (vocab_size C) at each time step (block_size T) in each batch (batch_size B)\n",
    "\n",
    "        # if we have targets, compute the CE loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # need to reshape for CE-loss in PyTorch \n",
    "            # (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # same shape as logits\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions (ignore the loss because we don't have targets)\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step = prediction for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C) instead of (B, T, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we sample one token at a time for each batch\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print('loss=', loss) \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start with a single token = 0 (idx = current context)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "# generate operates on batch level -> index into the 0th row = single batch dimension that exists -> one-dimensional array of all the indices (time steps)\n",
    "# afterwards convert to simple python list from tensor for decode function\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Go through the class definition above and explain what each function does! (1-2 sentences per function) **(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The `__init__()` function creates an embedding table out of the inputs. The `forward()` function calculates a forward pass through the network: It simply plucks out the row of the embedding matrix corresponding to the input character and interprets it as logits (weighted sum). Then it calculates a cross entropy loss using the labels. The `generate()` function applies softmax to the logits to get class probabilities, then samples the next character from a multinomial distribution based on these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How do you interpret the generated text? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** We see that the output is still nonsense because the model is untrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) What loss do you expect for this model? Can you compare the actual loss with your expectation? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The loss should be `-log(1/vocab_size)`, e.g. in the Shakespeare dataset, `-log(1/64) = 4.16`. The actual loss is a bit higher with a value of 5.04. This is due to a bit of randomness, not all tokens are equally likely at initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until now, the text history is not used, it is a simple bigram model (only the last character is used to predict the next one). Still, we feed in the whole sequence `xb`, `yb` up to `block_size` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5a) Create a PyTorch Adam optimizer with a learning rate of `1e-3`, pass it the model parameters for optimization (`model.parameters()`) and store it in `optimizer`. Check the documentation if needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=4.658271312713623\n",
      "step=100, loss=4.5111260414123535\n",
      "step=200, loss=4.465275764465332\n",
      "step=300, loss=4.2778754234313965\n",
      "step=400, loss=4.135361671447754\n",
      "step=500, loss=4.1009368896484375\n",
      "step=600, loss=4.087919235229492\n",
      "step=700, loss=3.9148900508880615\n",
      "step=800, loss=3.8526744842529297\n",
      "step=900, loss=3.7584140300750732\n",
      "step=1000, loss=3.7861006259918213\n",
      "step=1100, loss=3.563880205154419\n",
      "step=1200, loss=3.5855166912078857\n",
      "step=1300, loss=3.4968693256378174\n",
      "step=1400, loss=3.436549186706543\n",
      "step=1500, loss=3.426408290863037\n",
      "step=1600, loss=3.3652172088623047\n",
      "step=1700, loss=3.327918767929077\n",
      "step=1800, loss=3.194262742996216\n",
      "step=1900, loss=3.2044880390167236\n",
      "step=2000, loss=3.1342854499816895\n",
      "step=2100, loss=3.034728527069092\n",
      "step=2200, loss=2.9980807304382324\n",
      "step=2300, loss=3.070113182067871\n",
      "step=2400, loss=2.962170362472534\n",
      "step=2500, loss=3.0024642944335938\n",
      "step=2600, loss=2.826655626296997\n",
      "step=2700, loss=2.960874319076538\n",
      "step=2800, loss=2.9202170372009277\n",
      "step=2900, loss=2.8009681701660156\n",
      "step=3000, loss=2.658357620239258\n",
      "step=3100, loss=2.7676682472229004\n",
      "step=3200, loss=2.7083563804626465\n",
      "step=3300, loss=2.6876325607299805\n",
      "step=3400, loss=2.6991372108459473\n",
      "step=3500, loss=2.757277250289917\n",
      "step=3600, loss=2.6523561477661133\n",
      "step=3700, loss=2.6728813648223877\n",
      "step=3800, loss=2.5633881092071533\n",
      "step=3900, loss=2.6789321899414062\n",
      "step=4000, loss=2.6050984859466553\n",
      "step=4100, loss=2.6432387828826904\n",
      "step=4200, loss=2.5973660945892334\n",
      "step=4300, loss=2.590667963027954\n",
      "step=4400, loss=2.5424563884735107\n",
      "step=4500, loss=2.5252456665039062\n",
      "step=4600, loss=2.5899834632873535\n",
      "step=4700, loss=2.5105459690093994\n",
      "step=4800, loss=2.549785614013672\n",
      "step=4900, loss=2.5883522033691406\n",
      "step=5000, loss=2.5792126655578613\n",
      "step=5100, loss=2.5681862831115723\n",
      "step=5200, loss=2.5436134338378906\n",
      "step=5300, loss=2.4788663387298584\n",
      "step=5400, loss=2.5035758018493652\n",
      "step=5500, loss=2.5601792335510254\n",
      "step=5600, loss=2.5641133785247803\n",
      "step=5700, loss=2.6006503105163574\n",
      "step=5800, loss=2.3605799674987793\n",
      "step=5900, loss=2.476731300354004\n",
      "step=6000, loss=2.5388400554656982\n",
      "step=6100, loss=2.492650270462036\n",
      "step=6200, loss=2.4032280445098877\n",
      "step=6300, loss=2.4595885276794434\n",
      "step=6400, loss=2.5007128715515137\n",
      "step=6500, loss=2.434014320373535\n",
      "step=6600, loss=2.5419490337371826\n",
      "step=6700, loss=2.509185552597046\n",
      "step=6800, loss=2.59481143951416\n",
      "step=6900, loss=2.540569543838501\n",
      "step=7000, loss=2.433148145675659\n",
      "step=7100, loss=2.4574191570281982\n",
      "step=7200, loss=2.532711982727051\n",
      "step=7300, loss=2.425440788269043\n",
      "step=7400, loss=2.4213614463806152\n",
      "step=7500, loss=2.3525350093841553\n",
      "step=7600, loss=2.436553716659546\n",
      "step=7700, loss=2.4609334468841553\n",
      "step=7800, loss=2.4691004753112793\n",
      "step=7900, loss=2.478074073791504\n",
      "step=8000, loss=2.4081239700317383\n",
      "step=8100, loss=2.515564203262329\n",
      "step=8200, loss=2.398622512817383\n",
      "step=8300, loss=2.5117411613464355\n",
      "step=8400, loss=2.4670135974884033\n",
      "step=8500, loss=2.429988384246826\n",
      "step=8600, loss=2.4367454051971436\n",
      "step=8700, loss=2.4522688388824463\n",
      "step=8800, loss=2.649188995361328\n",
      "step=8900, loss=2.3619937896728516\n",
      "step=9000, loss=2.496692657470703\n",
      "step=9100, loss=2.4796195030212402\n",
      "step=9200, loss=2.498892307281494\n",
      "step=9300, loss=2.442564010620117\n",
      "step=9400, loss=2.461780309677124\n",
      "step=9500, loss=2.3577675819396973\n",
      "step=9600, loss=2.537917137145996\n",
      "step=9700, loss=2.550713539123535\n",
      "step=9800, loss=2.4117798805236816\n",
      "step=9900, loss=2.429363965988159\n",
      "2.5589075088500977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase batch size for better results\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # logits are not needed here\n",
    "    optimizer.zero_grad(set_to_none=True) # reset the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    # print the loss every 100 steps\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step={steps}, loss={loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate new text based on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "Ong h hasbe pave pirance\n",
      "RDe hicomyonthar's\n",
      "PES:\n",
      "AKEd ith henourzincenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KINld pe wither vouprroutherccnohathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so itJas\n",
      "Waketancotha:\n",
      "h hay.JUCLUKn prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZEESTEORDY:\n",
      "h l.\n",
      "KEONGBUCHandspo be y,-JZNEEYowddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghienHen yof GLANCHI me. strsithisgothers jveere!-e!\n",
      "QUCotouciullle's fld\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \")\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5b) How do you interpret the result? What could be a reason that the output is still suboptimal? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: The result looks a little bit better, but still suboptimal because we only used the last character for predicting the next, not the entire sequence. Next, all tokens should talk to each other for making better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized code so far (with some additions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "step 0: train loss 4.6715, val loss 4.6799\n",
      "step 300: train loss 2.8273, val loss 2.8328\n",
      "step 600: train loss 2.5454, val loss 2.5769\n",
      "step 900: train loss 2.5039, val loss 2.5136\n",
      "step 1200: train loss 2.4738, val loss 2.5101\n",
      "step 1500: train loss 2.4762, val loss 2.4967\n",
      "step 1800: train loss 2.4747, val loss 2.4883\n",
      "step 2100: train loss 2.4683, val loss 2.4861\n",
      "step 2400: train loss 2.4651, val loss 2.4890\n",
      "step 2700: train loss 2.4531, val loss 2.4977\n",
      "\n",
      "\n",
      "\n",
      "CAlors\n",
      "\n",
      "\n",
      "IORI athe, mare's'llabl qureamof ay\n",
      "TEO:chit, the, neads burily t, t Bos be, swsemaninconosim he!\n",
      "Plowit s ly pTouorie apiceelero\n",
      "I geandousie s h, serorknoloreth jelle agimuls me pandy.\n",
      "WVIA: bronch t:\n",
      "Wher wrkntononrou sousof had e'sasur t we, ou,\n",
      "Prd;\n",
      "BESes t d win,-f ulit ptr is h ss junore he O,\n",
      "ORGes's harewilley my MEN ugens hed nll von, fod.\n",
      "HAn d cees! tat it, pe's clle eruthinive, RTinus s seres thee l nisullime\n",
      "ak t wnthe tthipy od endoureveas fn:\n",
      "\n",
      "Son, then ly pare mfa tha\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # new: check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # new: we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # new: average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # new: switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # new: switch back to train mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "---\n",
    "## 6. The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now derive a more complex model that can look at all tokens at once to predict the next one, not just the last token. \n",
    "To use all previous tokens, the simplest idea is to use an average of all previous tokens. \n",
    "For example, the 5th token uses the **channels** (=feature maps, embeddings) of the 1st, 2nd, 3rd, 4th, and 5th token. \n",
    "The average of these is the **feature vector** for the 5th token and summarizes the context / history.\n",
    "Note that we have lost a lot of information, e.g. the order of the tokens, but it's a starting point. Consider the following toy example with batch size 4 , 8 tokens, 2 channels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # batch, time, channels. Goal: 8 tokens should talk to each other, but only from previous tokens, not from future tokens\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in each batch in the example vector `x`, we calculate the mean of the tokens that came before it in the time dimension (including itself). \n",
    "The result should be a tensor of shape (B,T,C) where the t-th row of the b-th batch contains the mean of all tokens in this batch that came before this token in the time dimension.\n",
    "We print the original tensor `x` and the resulting tensor `xbow` containing the mean values and make sure the mean values are correct. Here `bow` stands for **bag of words**, which means that each entry is an average of several words (each of the 8 tokens is considered a 'word' here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words = simple average of all previous tokens\n",
    "for b in range(B): # iterate over batch dimension\n",
    "    for t in range(T): # iterate over time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) # all previous tokens for this batch and time (slice)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # mean over time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9548,  0.9359],\n",
       "        [ 1.3888,  0.9721],\n",
       "        [-1.4494, -0.2664],\n",
       "        [-0.2139, -0.4374],\n",
       "        [-0.3362, -0.5552],\n",
       "        [ 1.4813, -0.3768],\n",
       "        [ 0.2222, -1.2649],\n",
       "        [-1.5363, -0.4280]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9548,  0.9359],\n",
       "        [ 1.6718,  0.9540],\n",
       "        [ 0.6314,  0.5472],\n",
       "        [ 0.4201,  0.3011],\n",
       "        [ 0.2688,  0.1298],\n",
       "        [ 0.4709,  0.0454],\n",
       "        [ 0.4354, -0.1418],\n",
       "        [ 0.1889, -0.1776]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # vertical average of all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using several nested loops like above, we use a trick with matrix multiplication that is mathematically equivalent but more efficient. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3) \n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, c contains the sum of the column entries of b. Because we only want the \"history\", not the \"future\" tokens to influence the result, we use a lower triangular matrix `a` instead, this is called **masking**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows, \n",
    "# third row is sum of all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to normalize for averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows + normalized, \n",
    "# third row is sum of all rows + normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6a) Now let's go back to our example above and apply the same trick. \n",
    "Define a lower triangular matrix called `wei` (previously `a`) that is normalized to sum to 1 along the rows. Matrix multiply `wei` with `x` to get a new matrix `xbow2`.\n",
    "Make sure that `xbow2` has the same shape as `x` and that it contains the correct values. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same result? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T)) # wei = weights (=a in the toy example)\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "# batched matrix multiplication (we apply matrix multiplication to each batch element separately)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) # c = a @ b in the toy example\n",
    "print('same result?', torch.allclose(xbow, xbow2)) # check if the two methods give the same result\n",
    "\n",
    "wei # average (sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9548,  0.9359],\n",
       "         [ 1.6718,  0.9540],\n",
       "         [ 0.6314,  0.5472],\n",
       "         [ 0.4201,  0.3011],\n",
       "         [ 0.2688,  0.1298],\n",
       "         [ 0.4709,  0.0454],\n",
       "         [ 0.4354, -0.1418],\n",
       "         [ 0.1889, -0.1776]]),\n",
       " tensor([[ 1.9548,  0.9359],\n",
       "         [ 1.6718,  0.9540],\n",
       "         [ 0.6314,  0.5472],\n",
       "         [ 0.4201,  0.3011],\n",
       "         [ 0.2688,  0.1298],\n",
       "         [ 0.4709,  0.0454],\n",
       "         [ 0.4354, -0.1418],\n",
       "         [ 0.1889, -0.1776]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xbow[0], xbow2[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6b) Now we use yet another mathematically equivalent way to compute the bag of words representation using **Softmax** function (this will be needed later for weighted sum instead of average of previous tokens).\n",
    "We start off with a lower triangular matrix where the lower triangle and diagonal is filled with 0, the upper with `-inf`. \n",
    "After applying the softmax function, the result will be again the `wei` matrix from before. Implement this in the following cell, calculate again the matrix multiplication of the new `wei` and `x` and check the result! **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix\n",
    "print(tril)\n",
    "\n",
    "wei = torch.zeros((T,T)) # preview: affinities are going to be dependent on context instead of zeroes\n",
    "# -> weighted aggregation in self-attention (tokens will find each other more or less interesting)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # make all future tokens -inf\n",
    "print(wei)\n",
    "\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize by row sum\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "print(wei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9548,  0.9359],\n",
       "         [ 1.6718,  0.9540],\n",
       "         [ 0.6314,  0.5472],\n",
       "         [ 0.4201,  0.3011],\n",
       "         [ 0.2688,  0.1298],\n",
       "         [ 0.4709,  0.0454],\n",
       "         [ 0.4354, -0.1418],\n",
       "         [ 0.1889, -0.1776]]),\n",
       " tensor([[ 1.9548,  0.9359],\n",
       "         [ 1.6718,  0.9540],\n",
       "         [ 0.6314,  0.5472],\n",
       "         [ 0.4201,  0.3011],\n",
       "         [ 0.2688,  0.1298],\n",
       "         [ 0.4709,  0.0454],\n",
       "         [ 0.4354, -0.1418],\n",
       "         [ 0.1889, -0.1776]]),\n",
       " tensor([[ 1.9548,  0.9359],\n",
       "         [ 1.6718,  0.9540],\n",
       "         [ 0.6314,  0.5472],\n",
       "         [ 0.4201,  0.3011],\n",
       "         [ 0.2688,  0.1298],\n",
       "         [ 0.4709,  0.0454],\n",
       "         [ 0.4354, -0.1418],\n",
       "         [ 0.1889, -0.1776]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Self-Attention\n",
    "\n",
    "Finally we get to the most important mechanism: **Self-Attention**! This will lead to a weighted average of the tokens (some tokens are more important than others to understand the text) instead of simply using the mean. And here is the idea: Every single token will emit two vectors: A **query** (\"What am I looking for?\") and a **key** (\"What do I contain?\"). The query then dot-products with all the keys to determine the similarity = affinity (stored in `wei`). Instead of the raw input `x`, which is private, a **value** is used (\"What will I communicate?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (increase channels for more interesting results)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # forward pass of x through the key layer\n",
    "q = query(x) # (B, T, 16) # forward pass of x through the query layer\n",
    "# so far, each token has a key and a query vector, no communication yet\n",
    "wei =  q @ k.transpose(-2, -1) # transpose last 2 dimensions (batch remains unchanged): (B, T, 16) @ (B, 16, T) ---> (B, T, T): for each batch, each token talks to all other tokens, so we get an affinity matrix of size (T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) # old version -> change to data dependent weights\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # comment to see intermediate results before normalization\n",
    "\n",
    "v = value(x) # we use the aggregated value instead of the raw x \n",
    "# x is private information to this token, v is the public information for communication\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) Print `wei` and compare it to the previous values. What is the most important change and why is this important here? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** Now `wei` is not constant anymore, but data dependent (every batch will have different weights because of different tokens)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the final entry 0.2391 is the weight for the 8th token. The 8th token emits a query ( for example \"I am a vowel at position 8, I am looking for consonants at positions up to 4\"). All tokens then emit keys, and maybe a consonant at position 4 will emit a key with high number in this channel, meaning \"I am a consonant at position 4\". The 8th token will therefore have a high weight for the 4th token (0.2297), resulting in a high affinity (dot product) - the 4th and 8th token \"have found each other\". Through the softmax function, a lot of information from the 4th token will be passed to the 8th token (meaning the 8th token will learn a lot from the 4th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "### Some Notes on Attention\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. Here we have `block_size = 8` nodes, where the first node is only pointed to by itself, the second by the first and itself, and so on. Attention can be applied to any directed graph, not only language modeling.\n",
    "- Each example across batch dimension is processed completely independently, the examples never \"talk\" to each other across different batches. The batched matrix multiplication above means applying matrix multiplication in parallel in each batch separately. For example here, you can think of 4 different graphs in parallel with 8 noded each, where the 8 nodes only communicate among each other, even though we process 32 nodes at once.\n",
    "- \"Scaled\" attention also divides `wei` by `1/sqrt(head_size)`, in the original paper:\n",
    "\\begin{equation*}\n",
    "   Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Without the normalization, using Gaussian input (zero mean and variance 1), the weights will be in the order of `head_size`. Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size) # k initialized from standard normal distribution (zero mean, unit variance)\n",
    "q = torch.randn(B,T,head_size) # q initialized from standard normal distribution (zero mean, unit variance)\n",
    "wei_unnormalized = q @ k.transpose(-2, -1) # will have variance of head_size roughly\n",
    "wei_normalized = q @ k.transpose(-2, -1)* head_size**-0.5 # normalize by sqrt of head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # variance of k: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() # variance of q: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4690)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(wei_unnormalized.var()) # variance of the dot product: roughly head_size=16\n",
    "print(wei_normalized.var()) # variance of the dot product: roughly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7b) Find out why this is important: Apply softmax to a tensor with entries around 0, then to another tensor with more extreme values. What happens? Write in the answer cell why we want to avoid this. **(2 points)**\n",
    "\n",
    "**HINT:** `torch.softmax()` expects an input specifying along which dimension to calculate the normalization (=which dimension should sum to 1), so you can pass `dim=-1` as second input for a 1D tensor. (See https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) # original example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2932e-04, 8.1630e-07, 1.7980e-02, 8.1630e-07, 9.8169e-01])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*20, dim=-1) # scaled by 20 -> output is more extreme, more confident, converging to one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The weights will feed into softmax. If we have rather big or small weights, the softmax will converge towards one-hot vectors, which means that we only use one input node and discard all others. This is not what we want, especially in the beginning when the network is untrained. So the variance should remain around 1 especially at initialization, meaning the weights should be fairly diffuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Encoding and Positional Encoding\n",
    "\n",
    "We will make one change on the token encoding: Previously, the `token_embedding_table` was of size `(vocab_size, vocab_size)`, which means we directly plucked out the logits from the embedding table. Now we want to introduce an intermediate layer (make the net bigger). Therefore, we introduce a new parameter `n_embd` for the number of embedding dimensions, for example we can choose 32 or 64 for this intermediate representation. So instead of logits, the `token_embedding_table` will give us **token embeddings**. These will be fed to a linear layer afterwards to get the logits:\n",
    "```\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary   \n",
    "```\n",
    "\n",
    "In the attention mechanism derived so far, there is no notion of space. Attention simply acts over a set of vectors. Remember that we can think of attention as a directed graph, where the nodes have no idea where they are positioned in a space. But space matters in text: For example, \"people love animals\" has a significantly different meaning than \"animals love people\", so the ordering of the words is very important. This is why we need to **positionally encode** tokens:\n",
    "So far, we have only encoded each token according to its identity `idx`. But we now also encode its position in a second embedding table: Each position from `0` to `block_size-1` will get its own embedding vector. This is the code snippet from the init function that we will implement below: \n",
    "```\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "```\n",
    "And here is a code snippet from the forward function, showing how integers from 0 to `block_size` are positionally encoded: \n",
    "```\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "```\n",
    "Right now, this is not useful yet, because we only use the last token in the Bigram model, so the position does not matter. But using attention, it will matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Single Self-Attention Head\n",
    "\n",
    "Now let's summarize the code so far and add a single self-attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "step 0: train loss 4.2272, val loss 4.2237\n",
      "step 500: train loss 2.6882, val loss 2.7151\n",
      "step 1000: train loss 2.5293, val loss 2.5306\n",
      "step 1500: train loss 2.4893, val loss 2.4842\n",
      "step 2000: train loss 2.4425, val loss 2.4499\n",
      "step 2500: train loss 2.4230, val loss 2.4356\n",
      "step 3000: train loss 2.4128, val loss 2.4248\n",
      "step 3500: train loss 2.3936, val loss 2.4271\n",
      "step 4000: train loss 2.3935, val loss 2.4096\n",
      "step 4500: train loss 2.3909, val loss 2.4098\n",
      "\n",
      "RI:\n",
      "ce spurre tor pelat shopaegaw It le alisthis lacllond urt areatt lulen en an por.\n",
      "\n",
      "'Hlel E: Nintthy, she inkn, thant utsplo'se pyot\n",
      "I ord pan fexangy-td uceamere, ther my'd, hyu sh ad\n",
      "bl ld han has youg.\n",
      "Ton it wad cerneces ainkeacthenar sthe hy mu, whe a prrint lt!\n",
      "\n",
      "BAP, qu beed the I ofr pe,\n",
      "\n",
      "AnTant angio yo pre'lle mithout wid:\n",
      "USo?\n",
      "\n",
      "Y:\n",
      "Tharse thouto slat bpiro go my ous dangonot waidel peeling?\n",
      "\n",
      "G wad, ade ffes, an ond Lait al at bean bsparincon bamure,\n",
      "Wis sou: ther\n",
      "D yoo makeelan yemy \n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # new: increase number of iterations due to lower learning rate\n",
    "eval_interval = 500 \n",
    "learning_rate = 1e-3 # new: lower learning rate (self-attention is more complex)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out\n",
    "\n",
    "# new: single self-attention head\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out \n",
    "    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # new: dimensionality of embeddings changed to n_embd as intermediate layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # new: position embeddings\n",
    "        self.sa_head = Head(n_embd) # new: self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # new: linear layer for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to the last block_size tokens (because we now use position embeddings, which only contain the last block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreased a bit, but the result is still not great. We will introduce some more changes following the transformer paper for further improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "---\n",
    "## 8. Full GPT Implementation\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "First, we add **multi-head attention**, which is simply several attention heads running in parallel, then concatenating the result over the channel dimension. A **projection layer** combines the concatenated outputs from all heads into a single unified representation and projects back to the original pathway. Note that \"projection\" in the context of Transformer models refers to a linear transformation that can either maintain, reduce, or even increase the dimensionality of the data. \n",
    "\n",
    "Intuitive Explanation: It helps to have multiple communication channels because these tokens have a lot to talk about - they want to find the consonants, the vowels, the vowels just from certain positions etc. and so it helps to create multiple independent channels of communication to gather lots of different types of data and then decode the output.\n",
    "\n",
    "<img src=\"multi-head-attention.jpg\" width=\"200\">\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "So far, we directly calculated the logits after the attention block, but this was way too fast - intuitively \"the tokens looked at each other, but didn't really have time to think on what they found from the other tokens\". Therefore, we add a feedforward single layer followed by a ReLU nonlinearity. Both layers together are called the **Transformer Block**, where we combine **communication** (self-attention) with **computation** (feedforward layer). This is on a per token level: Each token independently looks at the other tokens, and once it has gathered all the data, it thinks on that data individually. We implement this in the `Block` class below. The transformer block gets repeated over and over again.\n",
    "\n",
    "<img src=\"transformer.jpg\" width=\"300\">\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Also note that the transformer architecture above contains **skip connections (residual connections)**: The network contains parallel paths (one with some computations, one with the identity as \"shortcut\") that are combined via additions. Additions are great for backpropagation because they distribute gradients equally to both branches, so there is a \"shortcut\" for the gradients to directly propagate from the output to the input of the network. This avoids the vanishing gradient problem especially in the beginning - the transformer blocks only get more influence over time.\n",
    "\n",
    "\n",
    "<img src=\"skip-connection.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "\n",
    "The transformer architecture uses **layer norm** (called \"Norm\" in the architecture image above), which is very similar to **batch norm**: Batch norm makes sure that across the batch dimension, any individual neuron has unit gaussian distribution (zero mean, unit standard deviation). In layer norm, we don't normalize the columns, but the rows, which normalizes over layers instead of over batches:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}\\cdot \\gamma + \\beta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copied from BatchNorm1d in makemore series)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # previous batch mean -> index changed from 0 to 1\n",
    "    xvar = x.var(1, keepdim=True) # previous batch variance -> index changed from 0 to 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # no running mean and variance buffers needed like in batch norm\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8a) Check if mean and standard deviation of rows and/or columns are normalized now! Write the result in the answer cell. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1469) tensor(0.8803)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# columns are not normalized anymore:\n",
    "print(x[:,0].mean(), x[:,0].std()) # mean,std of one feature across all batch inputs\n",
    "\n",
    "# rows are normalized instead:\n",
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** We can see that rows are normalized now instead of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that layer norm is usually applied before the self-attention and linear layer nowadays (unlike the original paper) - one of the very few changes of the transformer architecture during the last years, otherwise mostly the architecture remained unchanged. This is called the **pre-norm formulation**. So here is a code snippet used below showing the two layer norms we will implement, one before the self-attention and one before the linear layer:\n",
    "\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x)) # layer norm directly applied to x before self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # layer norm applied before linear layer\n",
    "```\n",
    "\n",
    "Finally, another layer norm is typically applied at the end of the Transformer and right before the final linear layer that decodes into vocabulary. \n",
    "\n",
    "The size of the layer norm is `n_embds=32` here, so this is a per token transformation, it just normalizes the features and makes them unit Gaussian at initialization. Because these layer norms contain gamma and beta as trainable parameters, the layer norm may eventually create outputs that are not unit Gaussian depending on the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model\n",
    "\n",
    "We now have all components together so that we can scale up the model and make it bigger. Therefore, we add a parameter `n_layer=4` to specify that we want 4 transformer blocks.\n",
    "\n",
    "We also add **dropout** to prevent overfitting: with 4 transformer blocks, the network is getting quite large now. Therefore, we randomly deactivate some connections to prevent them from becoming too dominant. Because the mask of what's being dropped out has changed every single forward backward pass, effectively we end up training an ensemble of sub-networks. At test time, everything is fully enabled and all of those sub-networks are merged into a single ensemble, making it more robust.\n",
    "\n",
    "\n",
    "<img src=\"dropout.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPT with Multi-Head Attention and Transformer Block\n",
    "\n",
    "We finally get to the full GPT code, adding all the components explained above!\n",
    "\n",
    "**TODO:** 8b) In the summarized code below, comment each line to make sure you have understood all GPT components! You may use support from ChatGPT or GitHub Copilot, but double-check the results and be able to explain it yourself. (Yes, this is tedious, but it will help you get an in-depth understanding of the full GPT architecture) **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "10.788929 M parameters\n",
      "\n",
      "==============\n",
      "step 0: train loss 4.3684, val loss 4.3590\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "SaVqulef&&;giKUCZo\n",
      "\n",
      "D?QZFFmlHHkPkBj!'VTSbmRDynJeCHXEU.n;O,h&MHKRP\n",
      "\n",
      "qkjne AURe!PV$!EAyC Dbu$IbDKIGEff.Ca-F$SrYHIarCbcV\n",
      "xwkOu'Ww.WvxFxwFfRdI.LIzfmSBaw?ia\n",
      "Ib,3O3lE;maEwTcGGqmFlxHmshIqmUrObTsrcXwFz?agwGpP\n",
      "\n",
      "==============\n",
      "step 500: train loss 1.9145, val loss 2.0099\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Sisnding surncr: amest to him the and somen\n",
      "The will leve of of tousark, and cable.\n",
      "KIF dewastays le is biose tat will nitute?\n",
      "\n",
      "SOMENINE:\n",
      "O wire:\n",
      "Mum\n",
      "Fore my haveince dor mar magen shem cork pelque.\n",
      "\n",
      "\n",
      "\n",
      "==============\n",
      "step 1000: train loss 1.5540, val loss 1.7388\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "And VIVALINA:\n",
      "How now not bawight o'er wich make, away yetter,\n",
      "be deothernabe word make, our such rember.\n",
      "\n",
      "ServENERYn:\n",
      "I woman in the ind.\n",
      "Give me.\n",
      "No his, when dispeak tak the baste? me to but?\n",
      "\n",
      "MENE\n",
      "\n",
      "==============\n",
      "step 1500: train loss 1.4059, val loss 1.6210\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Mabrieven good thee once are to ajest witer.\n",
      "\n",
      "Third is me, this she children accide that our\n",
      "As in gracious would repretiest 'Tis subders\n",
      "My love thou sway busins kingsweem'ds\n",
      "Second new Bursolenes of\n",
      "\n",
      "==============\n",
      "step 2000: train loss 1.3216, val loss 1.5527\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "You are askep'd I; that when the mothers it\n",
      "Caise come desire not.\n",
      "\n",
      "FirsY:\n",
      "More melantain of the wellight not extain it;\n",
      "Too fied that reclove hellowled in Rather; and you\n",
      "S vent rinh: here it is the \n",
      "\n",
      "==============\n",
      "step 2500: train loss 1.2584, val loss 1.5159\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "\n",
      "LADY condits in the wadows more o'er-societ,\n",
      "To make from matching spoft conseally:\n",
      "Cousin to fight, that statle three see eyes,\n",
      "Which sronging in King Aumerle-cloud Grance to grave\n",
      "I thine exil good\n",
      "\n",
      "==============\n",
      "step 3000: train loss 1.2090, val loss 1.5021\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "ISABELLA:\n",
      "'Tis done.\n",
      "\n",
      "ISABELLA:\n",
      "Then not have I\n",
      "Wink upon Rome, what have devoure when\n",
      "I deny, I so, to look.\n",
      "This the Duke of You\n",
      "Of Gloucested pation's grave the grave, should,\n",
      "In Clarencements thee\n",
      "\n",
      "==============\n",
      "step 3500: train loss 1.1692, val loss 1.4893\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "By his friend absence. For; these took round it,\n",
      "And woful having Apollo satisfacted that them\n",
      "In his roar's.\n",
      "\n",
      "GLOUCESTER:\n",
      "Now told long me wrought\n",
      "You were in my hell. Prepened.\n",
      "\n",
      "PAULINA:\n",
      "But sir, Fr\n",
      "\n",
      "==============\n",
      "step 4000: train loss 1.1330, val loss 1.4835\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "Nay, after.\n",
      "\n",
      "Cloan:\n",
      "Well tell me, my lord anon.\n",
      "\n",
      "AUTOLYCUS:\n",
      "For 'tis not a place.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Nor sour man\n",
      "My chains, let's go: I have done in the sea,\n",
      "Or all woo' the receinest, harping \n",
      "\n",
      "==============\n",
      "step 4500: train loss 1.0970, val loss 1.4843\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "A protection of death as i' the mighties.\n",
      "\n",
      "GRUMIO:\n",
      "'Twere any of your bad, Your enemen,\n",
      "Well be patient brother, that you\n",
      "Have watered and your true-doubly; he says has\n",
      "pass'd and marrow you on Pace?\n",
      "\n",
      "\n",
      "==============\n",
      "step 4999: train loss 1.0646, val loss 1.4770\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Before him cause to troops as a lieure.\n",
      "\n",
      "Clorences:\n",
      "O my sorrow belly!\n",
      "Would have I do it, dead!\n",
      "\n",
      "LADY GREY:\n",
      "Many guard! away with me! So stars, Lord on, here,\n",
      "John hence, Mantua, revenge!\n",
      "What rather\n",
      "\n",
      "Final sample:\n",
      "\n",
      "BRUTUS:\n",
      "When be a tardines, awaked, what's you that we have.\n",
      "\n",
      "First Servingman:\n",
      "I'll resoluce the fool.\n",
      "\n",
      "MENENIUS:\n",
      "I carcle thee, 'omazed!\n",
      "\n",
      "COMINIUS:\n",
      "What music, I have took?\n",
      "\n",
      "LARTIUS:\n",
      "Not takenous no man?\n",
      "\n",
      "Messenger:\n",
      "From Above the robbost\n",
      "As that I be aslaid; it should choices the prince\n",
      "By bright was left in thee; but speaks up,\n",
      "Smeltles by woment, and soldier.\n",
      "The noble moves and toysier to the common\n",
      "To make them to clurish and pity; and, torraign doubme\n",
      "These eyes of yekeen parlous. Their arms, bawd gone!\n",
      "And get her in terrible! Alas, Tranio, for it!\n",
      "O thy needless rest any repetity.\n",
      "'Tis it all, that he scenews of sorrow'd devance\n",
      "The brlook's potector stone.\n",
      "\n",
      "ROMEO:\n",
      "Marcius; I'll try here until. Killy;\n",
      "That very all days i' them.\n",
      "\n",
      "FROMIS:\n",
      "I'! the day, and we'll.\n",
      "\n",
      "MERCUTIO:\n",
      "Where is all as 'twere event's butt the Volsce:\n",
      "Cry thou vaintain us tor me.\n",
      "\n",
      "VOLUMNIA:\n",
      "Twice present foot;\n",
      "grantity o'!\n",
      "\n",
      "CORIOLANUS:\n",
      "Let'st Warwick, that our common to guests Nora.\n",
      "\n",
      "Second Servant:\n",
      "More Clarence, Should Comellor:\n",
      "She best than Hastings: love-being and us,\n",
      "As come his, and from thereby time to Courts horse\n",
      "Of Mortar's?\n",
      "\n",
      "CAMILLA:\n",
      "My sound is the comfort of Coriolanus, it cany;\n",
      "Yours, even had urgent honour in't comfort: wherefolly,\n",
      "I have brazed you with him: I will do return it,\n",
      "And him quait to him: but, should by him sighing the oract,\n",
      "In provoks, and he tickly strokes, whom I accuring:\n",
      "List, put any excell woundest provent in one act.\n",
      "Besides, reHarry to them, I speak near to him.\n",
      "\n",
      "Nurse:\n",
      "I'll be the King Richmond; Cates; For she be fury\n",
      "sit of it booked by highway. Do your praise to see her,\n",
      "to see I come for praying the absents, he lord?\n",
      "\n",
      "PERDITA:\n",
      "He shall both\n",
      "Is by some unpossess'd: the quarrel of\n",
      "The officers of Aurion is rasple, whom comest modest grave\n",
      "In endure him scrafter'd me to all my true commits;\n",
      "And as All, senate os forth four you.\n",
      "Could he came them, as he losed on\n",
      "From His true darteres hence: whose I groan\n",
      "Toward them betwixt him be\n",
      "devised, or to be\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters version 1 - start here\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # how many training iterations?\n",
    "eval_interval = 500 # how often to evaluate the loss on train and val sets\n",
    "learning_rate = 1e-3 # learning rate\n",
    "eval_iters = 200 # how many samples to evaluate the loss on (result = mean over eval_iters)\n",
    "n_embd = 64 # embedding dimension\n",
    "n_head = 4 # number of heads in multi-head attention\n",
    "n_layer = 4 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate \n",
    "\n",
    "# hyperparameters version 2 - only uncomment when training on GPU\n",
    "#\"\"\"\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # how many training iterations?\n",
    "eval_interval = 500 # how often to evaluate the loss on train and val sets\n",
    "learning_rate = 3e-4 # learning rate\n",
    "eval_iters = 200 # how many samples to evaluate the loss on (result = mean over eval_iters)\n",
    "n_embd = 384 # embedding dimension\n",
    "n_head = 6 # number of heads in multi-head attention\n",
    "n_layer = 6 # number of transformer blocks\n",
    "dropout = 0.2 # dropout rate \n",
    "#\"\"\"\n",
    "\n",
    "# ------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
    "print('Running on device:',device) # print the device\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data # use the correct data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # sample random starting indices\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack them together\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # the targets are one token ahead\n",
    "    x, y = x.to(device), y.to(device) # move to GPU if available\n",
    "    return x, y # return the batch\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {} # store the results in this dictionary\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']: # evaluate both on train and val\n",
    "        losses = torch.zeros(eval_iters) # store the loss values\n",
    "        for k in range(eval_iters): # iterate over eval_iters\n",
    "            X, Y = get_batch(split) # get a batch\n",
    "            logits, loss = model(X, Y) # compute the loss \n",
    "            losses[k] = loss.item() # store the losses\n",
    "        out[split] = losses.mean() # store the mean loss in the dictionary\n",
    "    model.train() # switch back to train mode\n",
    "    return out # return the dictionary\n",
    "\n",
    "class Head(nn.Module): \n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size): # head_size = n_embd // n_head\n",
    "        super().__init__() # initialize the module\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularization\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        wei = self.dropout(wei) # apply dropout for regularization\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out # return the result\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size): \n",
    "        super().__init__() # initialize the module\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # create num_heads heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # project back to the original pathway\n",
    "        # Note: \"projection\" in the context of Transformer models refers to a linear transformation\n",
    "        # that can either maintain, reduce, or even increase the dimensionality of the data.\n",
    "        # The projection layer combines the concatenated outputs from all heads into a single unified representation.\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularization\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenate output of each head\n",
    "        out = self.dropout(self.proj(out)) # project back to the original pathway and apply dropout\n",
    "        return out # return the result\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd): # n_embd: embedding dimension (e.g., 64 or 384)\n",
    "        super().__init__() # initialize the module\n",
    "        self.net = nn.Sequential( # simple feed forward network\n",
    "            nn.Linear(n_embd, 4 * n_embd), # grow the hidden layer by a factor of 4 (see original paper)\n",
    "            nn.ReLU(), # ReLU activation function\n",
    "            nn.Linear(4 * n_embd, n_embd), # project back to the original pathway\n",
    "            nn.Dropout(dropout), # dropout layer for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # forward pass of the input x through the module\n",
    "        return self.net(x) # return the result\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head): # n_embd: embedding dimension, n_head: the number of heads we'd like \n",
    "        super().__init__() # initialize the module\n",
    "        head_size = n_embd // n_head # each head will have this size because we split the embedding into n_head parts\n",
    "        # (typical transformer structure: n_embd is divisible by n_head)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # self-attention layer\n",
    "        self.ffwd = FeedFoward(n_embd) # feed forward layer\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # layer norm for the self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd) # layer norm for the feed forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # typical transformer block: multi-head attention followed by feed forward including skip connections\n",
    "        # note that layer norm is applied before the self-attention nowadays (unlike the original paper)\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection + layer norm + self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection + layer norm + feed forward\n",
    "        return x\n",
    "\n",
    "# language model now using attention \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\" GPT language model with self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize the module\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # stack of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm (typically added after the transformer stack, right before the final linear layer that decodes into the vocabulary)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "        B, T = idx.shape # batch size and time\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) - get token embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1 for position embeddings\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        x = self.blocks(x) # (B,T,C) - transformer block\n",
    "        x = self.ln_f(x) # (B,T,C) - final layer norm\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size) - predict the logits\n",
    "\n",
    "        if targets is None: # during inference we only need the logits\n",
    "            loss = None # loss is therefore None\n",
    "        else:\n",
    "            B, T, C = logits.shape # store the shape of the logits\n",
    "            logits = logits.view(B*T, C) # reshape (B,T,C) -> (B*T,C) for cross entropy to work (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # reshape targets similarly\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy loss\n",
    "\n",
    "        return logits, loss # return the logits and loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # generate from the model\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # generate max_new_tokens\n",
    "            idx_cond = idx[:, -block_size:] # crop idx to the last block_size tokens (will run out of memory if bigger than block_size)\n",
    "            logits, loss = self(idx_cond)  # get the predictions\n",
    "            logits = logits[:, -1, :] # (B, C) - focus only on the last time step\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) - apply softmax to get probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) - sample from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) - append sampled index to the running sequence\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = GPTLanguageModel() # create the model\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # print the number of parameters in the model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # create a PyTorch optimizer\n",
    "\n",
    "for iter in range(max_iters): # iterate over the dataset\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1: #every once in a while evaluate the loss on train and val sets\n",
    "        losses = estimate_loss() # estimate the loss\n",
    "        print(\"\\n==============\")\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # print the results\n",
    "        print(\"==============\")\n",
    "        \n",
    "        # generate from the model\n",
    "        print(\"\\nSample:\")\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available (start with a single token = 0)\n",
    "        print(decode(model.generate(context, max_new_tokens=200)[0].tolist())) # generate a sample wit 200 tokens and decode it\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train') \n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # forward pass of the input through the model\n",
    "    optimizer.zero_grad(set_to_none=True) # set the gradients to zero\n",
    "    loss.backward() # backward pass to compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "# generate from the model\n",
    "print(\"\\nFinal sample:\") # generate a sample with 2000 tokens\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available (start with a single token = 0)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist())) # generate a sample with 2000 tokens and decode it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have trained a more powerful GPT model using self-attention. Let's generate a longer text and see how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ladam, and good word!\n",
      "Lay me, for that starts, break up him\n",
      "Unwilling: this ring wher I meet me fear\n",
      "At thy is't. Now! with I have heards thee,\n",
      "Or, if they know thee deadful asleep.\n",
      "\n",
      "POMPEY:\n",
      "I will for thee, pretty I shall; see themselves,\n",
      "As after her appire fetch and all.\n",
      "\n",
      "EXPOMARISA:\n",
      "I must, goodly cause.\n",
      "\n",
      "MPHMONDY:\n",
      "Patience, therefore the jauntion shall to me\n",
      "And tarrel to bed thy means, no lily especiat,\n",
      "Accurfit detress pluck'd up thee so ear.\n",
      "\n",
      "Second Lody Servingman:\n",
      "The glive you have for us excellent;\n",
      "I'll both death to Friar-place run;\n",
      "Yet she'll leader come me to to Rome; nepare you,\n",
      "Ress your couching and gifts, sir, here with dead,\n",
      "in Vient, all arge his hour's lot: comfort,\n",
      "For, a monstrous mouth should. Menly, look upon thee;\n",
      "Have I not successitted into us as be sauce,\n",
      "Nor old Frenzy, you do entern'd my dead.\n",
      "We wash that falls of golden sung.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Not might my good, imposity;\n",
      "Then the cease humours of despecting glory.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Will you go with be so wind them!\n",
      "\n",
      "BUCKINGHAM:\n",
      "Well, madam, as I thank to be;\n",
      "Gaunt, you, since, my lord, I would send him:\n",
      "I do it long, mine at it dash,\n",
      "I seon mine honour, then 'loves:'\n",
      "Long I would ride me, or as o'clocks my son.'\n",
      "\n",
      "AUTOLYCUS:\n",
      "In his clothes; haste from the nour tent?\n",
      "\n",
      "First Soldier:\n",
      "'Rescrect. Markets he doth and the pale: in him\n",
      "That he seech you have ere another no victory,\n",
      "Than see me his forceiving beaiseth gives up him honour.\n",
      "Now stand, camend thy daughter, prebardon,\n",
      "I'll not; but thou have revenued thy gave\n",
      "What shall be six. God-saves, his moyark! Delay,\n",
      "He dotier herself or aught and crop from our infrest,\n",
      "To sent the ryzput o' the Towers of sails dying,\n",
      "Even some scatters doth unto unjust me,\n",
      "As not minn'd thy hands between thee.\n",
      "\n",
      "LEONTES:\n",
      "You are not more in the spearer.\n",
      "\n",
      "PETER:\n",
      "Come, Mass, or none e'er beties grow.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I'll clip for his deck slander.\n",
      "What say Isabe: his nurse, for cames to him?\n",
      "There's not she; my noble lord command,\n",
      "Your voices is restoned palmner you actiful cause:\n",
      "Trature you shall have hear them as do go;\n",
      "And where he dotard as since we I do;\n",
      "And beseech you, poor-heads, the youngest colour,\n",
      "Piract of, and from't--weakness with your gracious;\n",
      "If he knows our their simper's friend,--hind were\n",
      "Your stoope's of her Iw, or evening, and in me\n",
      "To make her pingromity.\n",
      "\n",
      "ISABELLA:\n",
      "I would to seem less, it did.\n",
      "\n",
      "LUCIO:\n",
      "Trong to die secute our daughter,\n",
      "A majest and solton or mouth'd\n",
      "I in makes some a woman's earth:amontable too.\n",
      "Do you, how the care my marriage wears thanks, your lords\n",
      "At is trial. Alack, Lady comes one of my peace,\n",
      "And would quient me the people of dreamn's\n",
      "Of homerit your weaking words.\n",
      "\n",
      "LEONTES:\n",
      "O him!\n",
      "\n",
      "PAULINA:\n",
      "He men, to this expect him\n",
      "Is he dishonour that done:' goodly is in\n",
      "usent me in.\n",
      "\n",
      "POMPEY:\n",
      "Stand love you have pawdon us ten with me; Claudio,--\n",
      "\n",
      "CAMILLO:\n",
      "Come you toward best! We'll clamour you must perceive\n",
      "me where they had offend himself within.\n",
      "\n",
      "ISABELLA:\n",
      "I do not?\n",
      "\n",
      "ELBOW:\n",
      "But, Coriolane, violence:\n",
      "You sea-sing excranting to you.\n",
      "\n",
      "ANGELO:\n",
      "Self not another for sir, grant for forth. I out to serve\n",
      "proomish? I would not were with him the blocks\n",
      "claim there basen lutter'd you\n",
      "Of His dreamle traitory to minch for laws,\n",
      "Both fortunes of a looking found to accept his thumblance.\n",
      "These eyes of his been are simple for shine's teeth.'\n",
      "Rnoss and fast, he, at more joy\n",
      "I'd himself! She doth from the tide, means! Come, let's go:\n",
      "Norbid him!\n",
      "My gamer, that his gave my sickness me twight deech!\n",
      "And Gleomeness there stay surps my subjects;\n",
      "Enfrong'd upon, and my faught from himseance:\n",
      "Troub, troth-portent's voices more!\n",
      "\n",
      "Boy:\n",
      "Such nesdations\n",
      "The years,\n",
      "Would call'd out in her.\n",
      "\n",
      "VAUMILLINA, let him be my chance\n",
      "With his help'd and opinion.\n",
      "\n",
      "AUFIDIUS:\n",
      "Say no near!\n",
      "\n",
      "BRUTUS:\n",
      "Lord Abhorse, hear! Sir, for please Isabs,\n",
      "The Marcius abhoar, that makes him with thee\n",
      "By her behead the other ky\n",
      "By what the danger on his own.\n",
      "\n",
      "Clown:\n",
      "Well, you owe or Thursday decerning, how the marries\n",
      "We here theyhald confrance, the bear of Crudais' mother;\n",
      "Else with the diate of France;\n",
      "With poising, like thee stipps for the fanciners,\n",
      "Thou ne'er very berecry. Heatens!\n",
      "Thy manatics have with me endured; foreside\n",
      "That our boness and murder's daughtering and her ob.\n",
      "\n",
      "Lord:\n",
      "Master, I pound, thy delay accorsed thee not;\n",
      "Not now-bray thy men black story true duke\n",
      "In thy laste on joyfeitty head infair\n",
      "This report of these sheperaters plead bears them to Romvens,\n",
      "Too rose, I hear those to shower womb and swords;\n",
      "That look I their love at evermous rength.\n",
      "\n",
      "HASTINGS:\n",
      "Now impossession of me::\n",
      "Madam, is Coriolanus made\n",
      "Hath done those blown with them her lies! They shall not so say,\n",
      "Their wretchedness whose than my shamper'd.\n",
      "\n",
      "HORTENSIO:\n",
      "As he took LAND:\n",
      "Ay, the tops to miss them good worship,\n",
      "And have stumberranted up them, Evils and when\n",
      "The sea of battering soft.\n",
      "\n",
      "CAMILLO:\n",
      "How now, Master comenargue the mannor\n",
      "Their herself. Therefore even committed to him!\n",
      "The gateer or thereof lions.\n",
      "But he, must he mights in their lade,\n",
      "One moneral or changes of their saints.\n",
      "Have help me once, let me, I'mmade them to go.\n",
      "\n",
      "ISABELLA:\n",
      "Sir, Death, my infant names is lord; almouring,'\n",
      "So, sir, boys'd us sleep; that has dones,\n",
      "That three nursed away?\n",
      "With unaptured in thine, and make them baid not?\n",
      "\n",
      "ISABELLA:\n",
      "The object bids, mutinous trumpt speak pating doth\n",
      "That makes of glass profass;\n",
      "Or desperate and Bolio one town:\n",
      "From instruments, how and could burns the England\n",
      "Of Weddeder, wilnes, Warwick is play too,\n",
      "Thou wander to Covert:\n",
      "Some, to-morrow.\n",
      "\n",
      "POLIXERONE:\n",
      "It should yet be so.\n",
      "\n",
      "Second Servent:\n",
      "She dudges is at his chaufiders.\n",
      "\n",
      "CAMILLO:\n",
      "But, My lord, you didst know,\n",
      "But she's him any parture music: he had not\n",
      "For them been with her tongue.\n",
      "\n",
      "COMINIUS:\n",
      "So of all her: a most only with him!\n",
      "\n",
      "CORIOLANUS:\n",
      "Almost home:\n",
      "You do mock'd with you.\n",
      "\n",
      "MENENIUS:\n",
      "Here, Aupolla.\n",
      "\n",
      "CORIOLANUS:\n",
      "Come, I hear the issue. Or\n",
      "MENENIUS:\n",
      "Ha! come, comfort! thy boy brows taken, to't;\n",
      "\n",
      "AUFIDIUS:\n",
      "Suppose;\n",
      "Commune not beiling.\n",
      "Eithert call hers, well accompative\n",
      "That inquirous all myself,\n",
      "To resist a troublor thee can: I do dare believe:\n",
      "Do not less\n",
      "Indeed and bereft them them and for mean.\n",
      "Those services bears to Good, I beseech you\n",
      "Advant, to Coriolanus:\n",
      "Besides, I cannot we worn the maid,\n",
      "Wrath, bier the necess'd of thee.\n",
      "\n",
      "Shepherd:\n",
      "What of them? O challs! homerous!\n",
      "\n",
      "Second Murderer:\n",
      "No, for God!\n",
      "\n",
      "First Servant:\n",
      "I'll be here a kind of the presence.\n",
      "\n",
      "AUTOLYCUS:\n",
      "A letter even's since\n",
      "LOkes, are these friar; but a bad, a bawd words at liquit,\n",
      "For 'na, like his; for once teeps o'er doom\n",
      "O' deviles, so be old awry shall I.\n",
      "\n",
      "Second Senator:\n",
      "Heavens much never so? So the governess, ere you mates\n",
      "Shall show it out.\n",
      "\n",
      "AUFIDIUS:\n",
      "Now The queen's head: there's this woman haxh\n",
      "that Dy Son, which brooched in blood,\n",
      "and to big heaven of this ready ends\n",
      "Norture that yet she makes with child heart\n",
      "Where sleep wish toat me once,\n",
      "And with of the father: smeants, and we meet should sea\n",
      "Hunts, but me and ruly servant; we must thou pity upon\n",
      "Thy courage and syllable about my coulderful blood\n",
      "To into the servail of York; my lord stamps silent.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Mariage and to jewel and book.\n",
      "But in our cution can is graced again.\n",
      "\n",
      "QUEEN ELIO:\n",
      "Trot, of these word of merry beauty father\n",
      "Towards Leasuy depose.\n",
      "\n",
      "PAULINA:\n",
      "O me, ye\n",
      "Do expect me no mistresses, then both and their man\n",
      "Unto the broken of friends\n",
      "To my lawful degree.\n",
      "\n",
      "BENVOLIO:\n",
      "Be you may: I'll be not fight again.\n",
      "\n",
      "Messenger:\n",
      "You are you suit for themselves? I have\n",
      "More than evail of you.\n",
      "\n",
      "PERDITA:\n",
      "Is to Londoers.\n",
      "\n",
      "ANTIGONUS:\n",
      "It be, for my lord.\n",
      "\n",
      "Officer:\n",
      "And the commons good prayers, royal Varlius,\n",
      "Have I livet in joyful and and lazyard's sword;\n",
      "Were not 'Ten 'I shall no greaty gave:\n",
      "That now to you folly yours wonder,\n",
      "No wonder was, though book'd young, but may hand:\n",
      "Then not, 'tis the made, or knee, my minifer,\n",
      "In with a paratish'd bystering weaking:\n",
      "Thou hast in him, proud a copel terEach'd odd!\n",
      "\n",
      "CAPULET:\n",
      "Thou must lie some be matroisons much\n",
      "Be detect eShed hither.\n",
      "\n",
      "GS:\n",
      "Go to the Baptista that present of unfalse my jade,\n",
      "Shall be so never stroke immocup!\n",
      "Married me to brid the foot of him much in abject;\n",
      "Not do I see the access my mother, daughter,\n",
      "I shall be so enjointed forsworn to die.\n",
      "\n",
      "DUKES:\n",
      "Laim, I tell your fair,\n",
      "That I give me to do set down with your revenge,\n",
      "And since me but that preverein peace:\n",
      "Bid my love sir, to rie and breathe your accusation.\n",
      "Hae I been my purpose to sight.\n",
      "\n",
      "ISABELLA:\n",
      "For favour comfort and let:\n",
      "See brat is not milliks, but anon.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Say now your conscience as I trick my brother:\n",
      "I know it to your interplance.\n",
      "\n",
      "DUKE VERMARIANE:\n",
      "Now my Northum will enving my tentage.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What is't her cause me? Juielly bain!\n",
      "Then I am I stoop to my brother below'd in hope,\n",
      "So fray not for revenuic near,\n",
      "Pursues to see how to reap, my words to excuse much.\n",
      "And once I am let to see thy heart?\n",
      "So, here will I say '? Alas!\n",
      "To call more and loouth.' O, poor Menenius!'?\n",
      "And must thyself a fire?\n",
      "Let's mew should have it see them but at Oxford?\n",
      "And here left to her due my burious;\n",
      "And, this minet! Tell her teeth shers of rushelves,\n",
      "Fresh on him and my broison towards harm;\n",
      "And in the excration with all the sun,\n",
      "And in dogs here in his humorous.\n",
      "\n",
      "JULIET:\n",
      "Gave hites; toward in thirt, he shall we doe.\n",
      "More, I live together, in my low.\n",
      "\n",
      "Nurse:\n",
      "Ay,'tis a better.\n",
      "\n",
      "LEONTES:\n",
      "If; come.\n",
      "His man: do I hear aid.\n",
      "\n",
      "PARIS:\n",
      "Harry's her, here's; tarraigh, not Mercure, I meanly.\n",
      "\n",
      "PAULINA:\n",
      "Nor, pray, a say you husband. As mert of her,\n",
      "That fares you neither discourciusic,\n",
      "Corious! The other of Rome accusation. Darise\n",
      "Here cratuives leads your drees and gracts:\n",
      "Thou'rt a proveth their cords themselves\n",
      "Suppen them yet up become, stop with that good\n",
      "More intouchely buty or barriage:\n",
      "I shall to knolly sovereign my decertaim',\n",
      "Cry my lord and tears of heaven him.\n",
      "\n",
      "LEONTES:\n",
      "What!\n",
      "Broken, purceive I would on both son;\n",
      "Speak your services moeth? doth you.\n",
      "\n",
      "LADY CAPULET:\n",
      "O, why, an it becomes, I am motio;\n",
      "But to purz\n"
     ]
    }
   ],
   "source": [
    "# generate a longer sample\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a text file\n",
    "f =  open(\"GPT_generated_text.txt\",\"w\")\n",
    "f.write(new_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Apply the code to a different text of your choice! What loss do you achieve? What parameters did you change and why? How do you interpret the output compared to the Shakespeare output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Next Steps\n",
    "### Andrej's Suggested Further Experiments\n",
    "\n",
    "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder and Encoder\n",
    "\n",
    "Text generation as above only uses the **decoder** part of the transformer architecture. The **decoder attention block** implemented above has **triangular masking**, and is usually used in autoregressive settings, like language modeling. \n",
    "\n",
    "In other settings, we do want \"future\" tokens to influence the prediction, so we do not use triangular masking. For example, in sentiment analysis, we look at a whole sentence at once, then predict the sentiment \"happy\" or \"sad\" of the speaker. This can be realized using an **encoder** attention block. To implement an encoder attention block, we can simply delete the single line that does masking with `tril`, allowing all tokens to communicate. Attention does not care whether tokens from the future contribute or not, it supports arbitrary connectivity between nodes.\n",
    "\n",
    "### From Self-Attention to Cross-Attention\n",
    "\n",
    "**Self-attention** means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module). For example, when translating from French to English, we condition the decoding on the past decoding *and* the fully encoded french prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GPT to ChatGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a long way to go from our toy GPT example to ChatGPT. \n",
    "First of all, ChatGPT's **pre-training** was done on a large chunk of internet, resulting in a decoder-only transformer for text generation. \n",
    "So the pretraining is quite similar to our toy example training, except that we used roughly 10 million parameters and the largest transformer for ChatGPT uses 175 billion (!) parameters. Also it was trained on 300 billion tokens (our training set would be 300.000 tokens roughly when not using character-level tokens, but sub-word chunks). This is about a million fold increase in number of tokens - and today, even bigger datasets are used with trillions of tokens for training on thousands of GPUs!\n",
    "\n",
    "See the following table for the number of parameters, number of layers, n_embd, number of heads, head size, batch size and learning rate in **GPT-3**:\n",
    "\n",
    "\n",
    "<img src=\"GPT3_params_table.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-training, the model will be a document completer, it will not give answers but produce more questions or result in some undefined behavior. For becoming an assistant, further **fine-tuning** is needed using **Reinforcement Learning from Human Feedback (RLHF)**. Here is an overview of manual fine-tuning with human AI trainers (see the OpenAI ChatGPT blog for details, link below):\n",
    "\n",
    "<img src=\"chatgpt_diagram_light.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To sum it up, we trained a decoder only Transformer following the famous paper 'Attention is All You Need' from 2017, which is basically a GPT. We saw how using self-attention, we can calculate a weighted average of past tokens to predict the next token. We trained it on different texts (Shakespeare, Faust, Jane Austen etc.) and produced new texts in the same writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 \n",
    "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell truncates long output to a maximum length, then converts the notebook to a PDF for submission\n",
    "# NOTE: You may have to adapt the path or filename to match your local setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# truncate long cell output to avoid large pdf files\n",
    "from helpers.truncate_output import truncate_long_notebook_output\n",
    "truncated = truncate_long_notebook_output('3_Character_Level_GPT__solution.ipynb')\n",
    "\n",
    "# convert to pdf with nbconvert\n",
    "if truncated:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download TRUNCATED_3_Character_Level_GPT__solution.ipynb\n",
    "else:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download 3_Character_Level_GPT__solution.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
